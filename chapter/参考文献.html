<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>参考文献 &#8212; 人工智能内生安全 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="shortcut icon" href="../_static/logo.png?"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="8. 总结与展望" href="chap-8.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">参考文献</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter/参考文献.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/AI-safety-book">
                  <i class="fab fa-github"></i>
                  Github
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo_with_text.png" alt="人工智能内生安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<!-- <li class="toctree-l1"><a class="reference internal" href="about.html">关于此书</a></li> -->
<li class="toctree-l1"><a class="reference internal" href="%E5%89%8D%E8%A8%80.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7%E8%A1%A8.html">常用符号表</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="chap-1.html">1. 人工智能安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-2.html">2. 生命周期</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-3.html">3. 攻击算法基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-4.html">4. 防御算法基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-5.html">5. 视觉大模型安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-6.html">6. 大语言模型安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-7.html">7. 多模态大模型安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-8.html">8. 总结与展望</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo_with_text.png" alt="人工智能内生安全"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<!-- <li class="toctree-l1"><a class="reference internal" href="about.html">关于此书</a></li> -->
<li class="toctree-l1"><a class="reference internal" href="%E5%89%8D%E8%A8%80.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7%E8%A1%A8.html">常用符号表</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="chap-1.html">1. 人工智能安全概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-2.html">2. 生命周期</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-3.html">3. 攻击算法基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-4.html">4. 防御算法基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-5.html">5. 视觉大模型安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-6.html">6. 大语言模型安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-7.html">7. 多模态大模型安全</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap-8.html">8. 总结与展望</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1>参考文献<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<div class="docutils container" id="id2">
<dl class="citation">
<dt class="label" id="id141"><span class="brackets">Abadi et al., 2016</span></dt>
<dd><p>Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., &amp; Zhang, L. (2016). Deep learning with differential privacy. <em>ACM SIGSAC Conference on Computer and Communications Security</em>.</p>
</dd>
<dt class="label" id="id293"><span class="brackets">Abnar &amp; Zuidema, 2020</span></dt>
<dd><p>Abnar, S., &amp; Zuidema, W. (2020). Quantifying attention flow in transformers. <em>Annual Meeting of the Association for Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">Achiam et al., 2023</span></dt>
<dd><p>Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., … others. (2023). Gpt-4 technical report. <em>arXiv preprint arXiv:2303.08774</em>.</p>
</dd>
<dt class="label" id="id142"><span class="brackets">Adi et al., 2018</span></dt>
<dd><p>Adi, Y., Baum, C., Cisse, M., Pinkas, B., &amp; Keshet, J. (2018). Turning your weakness into a strength: watermarking deep neural networks by backdooring. <em>USENIX Security Symposium</em>.</p>
</dd>
<dt class="label" id="id143"><span class="brackets">Alayrac et al., 2019</span></dt>
<dd><p>Alayrac, J.-B., Uesato, J., Huang, P.-S., Fawzi, A., Stanforth, R., &amp; Kohli, P. (2019). Are labels required for improving adversarial robustness? <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id414"><span class="brackets">An et al., 2024</span></dt>
<dd><p>An, S., Chou, S.-Y., Zhang, K., Xu, Q., Tao, G., Shen, G., … others. (2024). Elijah: eliminating backdoors injected in diffusion models via distribution shift. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id409"><span class="brackets">Anderberg et al., 2024</span></dt>
<dd><p>Anderberg, A., Bailey, J., Campello, R. J., Houle, M. E., Marques, H. O., Radovanović, M., &amp; Zimek, A. (2024). Dimensionality-aware outlier detection: theoretical and experimental analysis. <em>SIAM International Conference on Data Mining</em>.</p>
</dd>
<dt class="label" id="id144"><span class="brackets">Andreina et al., 2021</span></dt>
<dd><p>Andreina, S., Marson, G. A., Möllering, H., &amp; Karame, G. (2021). Baffle: backdoor detection via feedback-based federated learning. <em>International Conference on Distributed Computing Systems</em>.</p>
</dd>
<dt class="label" id="id74"><span class="brackets">Andriushchenko et al., 2020</span></dt>
<dd><p>Andriushchenko, M., Croce, F., Flammarion, N., &amp; Hein, M. (2020). Square attack: a query-efficient black-box adversarial attack via random search. <em>European Conference on Computer Vision</em> (pp. 484–501).</p>
</dd>
<dt class="label" id="id145"><span class="brackets">Athalye et al., 2018a</span></dt>
<dd><p>Athalye, A., Carlini, N., &amp; Wagner, D. (2018). Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples. <em>International Conference on Machine Learning</em> (pp. 274–283).</p>
</dd>
<dt class="label" id="id85"><span class="brackets">Athalye et al., 2018b</span></dt>
<dd><p>Athalye, A., Engstrom, L., Ilyas, A., &amp; Kwok, K. (2018). Synthesizing robust adversarial examples. <em>International Conference on Machine Learning</em> (pp. 284–293).</p>
</dd>
<dt class="label" id="id111"><span class="brackets">Bagdasaryan et al., 2020</span></dt>
<dd><p>Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., &amp; Shmatikov, V. (2020). How to backdoor federated learning. <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 2938–2948).</p>
</dd>
<dt class="label" id="id363"><span class="brackets">Bai et al., 2024</span></dt>
<dd><p>Bai, Y., Pei, G., Gu, J., Yang, Y., &amp; Ma, X. (2024). Special characters attack: toward scalable training data extraction from large language models. <em>arXiv preprint arXiv:2405.05990</em>.</p>
</dd>
<dt class="label" id="id146"><span class="brackets">Bai et al., 2020</span></dt>
<dd><p>Bai, Y., Zeng, Y., Jiang, Y., Xia, S.-T., Ma, X., &amp; Wang, Y. (2020). Improving adversarial robustness via channel-wise activation suppressing. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id365"><span class="brackets">Bai et al., 2022a</span></dt>
<dd><p>Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., … others. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback. <em>arXiv preprint arXiv:2204.05862</em>.</p>
</dd>
<dt class="label" id="id322"><span class="brackets">Bai et al., 2022b</span></dt>
<dd><p>Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., … others. (2022). Constitutional ai: harmlessness from ai feedback. <em>arXiv preprint arXiv:2212.08073</em>.</p>
</dd>
<dt class="label" id="id274"><span class="brackets">Ban &amp; Dong, 2022</span></dt>
<dd><p>Ban, Y., &amp; Dong, Y. (2022). Pre-trained adversarial perturbations. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id400"><span class="brackets">Bansal et al., 2023</span></dt>
<dd><p>Bansal, H., Singhi, N., Yang, Y., Yin, F., Grover, A., &amp; Chang, K.-W. (2023). Cleanclip: mitigating data poisoning attacks in multimodal contrastive learning. <em>IEEE/CVF International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id372"><span class="brackets">Bao et al., 2023</span></dt>
<dd><p>Bao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., … Zhu, J. (2023). One transformer fits all distributions in multi-modal diffusion at scale. <em>International Conference on Machine Learning</em> (pp. 1692–1717).</p>
</dd>
<dt class="label" id="id90"><span class="brackets">Barreno et al., 2006</span></dt>
<dd><p>Barreno, M., Nelson, B., Sears, R., Joseph, A. D., &amp; Tygar, J. D. (2006). Can machine learning be secure? <em>ACM Symposium on Information, Computer and Communications Security</em> (pp. 16–25).</p>
</dd>
<dt class="label" id="id148"><span class="brackets">Bendale &amp; Boult, 2016</span></dt>
<dd><p>Bendale, A., &amp; Boult, T. E. (2016). Towards open set deep networks. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 1563–1572).</p>
</dd>
<dt class="label" id="id62"><span class="brackets">Biggio et al., 2013</span></dt>
<dd><p>Biggio, B., Corona, I., Maiorca, D., Nelson, B., Šrndić, N., Laskov, P., … Roli, F. (2013). Evasion attacks against machine learning at test time. <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em> (pp. 387–402).</p>
</dd>
<dt class="label" id="id92"><span class="brackets">Biggio et al., 2012</span></dt>
<dd><p>Biggio, B., Nelson, B., &amp; Laskov, P. (2012). Poisoning attacks against support vector machines. <em>International Conference on International Conference on Machine Learning</em> (pp. 1467–1474). Madison, WI, USA: Omnipress.</p>
</dd>
<dt class="label" id="id149"><span class="brackets">Blanchard et al., 2017</span></dt>
<dd><p>Blanchard, P., El Mhamdi, E. M., Guerraoui, R., &amp; Stainer, J. (2017). Machine learning with adversaries: byzantine tolerant gradient descent. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">Brendel et al., 2018</span></dt>
<dd><p>Brendel, W., Rauber, J., &amp; Bethge, M. (2018). Decision-based adversarial attacks: reliable attacks against black-box machine learning models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">Brown et al., 2020</span></dt>
<dd><p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., … others. (2020). Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id84"><span class="brackets">Brown et al., 2017</span></dt>
<dd><p>Brown, T. B., Mané, D., Roy, A., Abadi, M., &amp; Gilmer, J. (2017). <em>Adversarial patch</em>.</p>
</dd>
<dt class="label" id="id152"><span class="brackets">Cai et al., 2018</span></dt>
<dd><p>Cai, Q.-Z., Liu, C., &amp; Song, D. (2018). Curriculum adversarial training. <em>International Joint Conference on Artificial Intelligence</em> (pp. 3740–3747).</p>
</dd>
<dt class="label" id="id153"><span class="brackets">Cao et al., 2021a</span></dt>
<dd><p>Cao, X., Jia, J., &amp; Gong, N. Z. (2021). Ipguard: protecting intellectual property of deep neural networks via fingerprinting the classification boundary. <em>ACM Asia Conference on Computer and Communications Security</em>.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">Cao et al., 2021b</span></dt>
<dd><p>Cao, Y., Wang, N., Xiao, C., Yang, D., Fang, J., Yang, R., … Li, B. (2021). Invisible for both camera and lidar: security of multi-sensor fusion based perception in autonomous driving under physical-world attacks. <em>IEEE Symposium on Security and Privacy</em>.</p>
</dd>
<dt class="label" id="id355"><span class="brackets">Carlini et al., 2022</span></dt>
<dd><p>Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., &amp; Tramer, F. (2022). Membership inference attacks from first principles. <em>IEEE Symposium on Security and Privacy</em>.</p>
</dd>
<dt class="label" id="id332"><span class="brackets">Carlini et al., 2023a</span></dt>
<dd><p>Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., &amp; Zhang, C. (2023). Quantifying memorization across neural language models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id383"><span class="brackets">Carlini et al., 2023b</span></dt>
<dd><p>Carlini, N., Nasr, M., Choquette-Choo, C. A., Jagielski, M., Gao, I., Awadalla, A., … others. (2023). Are aligned neural networks adversarially aligned? <em>arXiv:2306.15447</em>.</p>
</dd>
<dt class="label" id="id395"><span class="brackets">Carlini &amp; Terzis, 2021</span></dt>
<dd><p>Carlini, N., &amp; Terzis, A. (2021). Poisoning and backdooring contrastive learning. <em>arXiv preprint arXiv:2106.09667</em>.</p>
</dd>
<dt class="label" id="id334"><span class="brackets">Carlini et al., 2021</span></dt>
<dd><p>Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … others. (2021). Extracting training data from large language models. <em>USENIX Security Symposium</em> (pp. 2633–2650).</p>
</dd>
<dt class="label" id="id155"><span class="brackets">Carlini &amp; Wagner, 2017a</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2017). Adversarial examples are not easily detected: bypassing ten detection methods. <em>ACM Workshop on Artificial Intelligence and Security</em> (pp. 3–14).</p>
</dd>
<dt class="label" id="id156"><span class="brackets">Carlini &amp; Wagner, 2017b</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2017). Magnet and&quot; efficient defenses against adversarial attacks&quot; are not robust to adversarial examples. <em>arXiv preprint arXiv:1711.08478</em>.</p>
</dd>
<dt class="label" id="id70"><span class="brackets">Carlini &amp; Wagner, 2017c</span></dt>
<dd><p>Carlini, N., &amp; Wagner, D. (2017). Towards evaluating the robustness of neural networks. <em>IEEE Symposium on Security and Privacy</em> (pp. 39–57).</p>
</dd>
<dt class="label" id="id339"><span class="brackets">Carlini et al., 2023c</span></dt>
<dd><p>Carlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, V., Tramer, F., … Wallace, E. (2023). Extracting training data from diffusion models. <em>USENIX Security Symposium</em>.</p>
</dd>
<dt class="label" id="id157"><span class="brackets">Carmon et al., 2019</span></dt>
<dd><p>Carmon, Y., Raghunathan, A., Schmidt, L., Duchi, J. C., &amp; Liang, P. S. (2019). Unlabeled data improves adversarial robustness. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id158"><span class="brackets">Chan et al., 2022</span></dt>
<dd><p>Chan, S.-H., Dong, Y., Zhu, J., Zhang, X., &amp; Zhou, J. (2022). <em>BadDet: Backdoor Attacks on Object Detection</em>.</p>
</dd>
<dt class="label" id="id159"><span class="brackets">Chang et al., 2000</span></dt>
<dd><p>Chang, S. G., Yu, B., &amp; Vetterli, M. (2000). Adaptive wavelet thresholding for image denoising and compression. <em>IEEE Transactions on Image Processing</em>, <em>9</em>(9), 1532–1546.</p>
</dd>
<dt class="label" id="id347"><span class="brackets">Chao et al., 2023</span></dt>
<dd><p>Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G. J., &amp; Wong, E. (2023). Jailbreaking black box large language models in twenty queries. <em>arXiv preprint arXiv:2310.08419</em>.</p>
</dd>
<dt class="label" id="id160"><span class="brackets">Chaudhuri &amp; Monteleoni, 2008</span></dt>
<dd><p>Chaudhuri, K., &amp; Monteleoni, C. (2008). Privacy-preserving logistic regression. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id161"><span class="brackets">Chen et al., 2018</span></dt>
<dd><p>Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., … Srivastava, B. (2018). <em>Detecting backdoor attacks on deep neural networks by activation clustering</em>.</p>
</dd>
<dt class="label" id="id162"><span class="brackets">Chen et al., 2019</span></dt>
<dd><p>Chen, H., Fu, C., Zhao, J., &amp; Koushanfar, F. (2019). Deepinspect: a black-box trojan detection and mitigation framework for deep neural networks. <em>International Joint Conference on Artificial Intelligence</em> (pp. 4658–4664).</p>
</dd>
<dt class="label" id="id164"><span class="brackets">Chen et al., 2022a</span></dt>
<dd><p>Chen, J., Wang, J., Peng, T., Sun, Y., Cheng, P., Ji, S., … Song, D. (2022). Copy, right? a testing framework for copyright protection of deep learning models. <em>IEEE Symposium on Security and Privacy</em>.</p>
</dd>
<dt class="label" id="id75"><span class="brackets">Chen et al., 2017a</span></dt>
<dd><p>Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., &amp; Hsieh, C.-J. (2017). Zoo: zeroth order optimization based black-box attacks to deep neural networks without training substitute models. <em>ACM Workshop on Artificial Intelligence and Security</em> (pp. 15–26).</p>
</dd>
<dt class="label" id="id354"><span class="brackets">Chen et al., 2022b</span></dt>
<dd><p>Chen, S., Liu, C., Haque, M., Song, Z., &amp; Yang, W. (2022). Nmtsloth: understanding and testing efficiency degradation of neural machine translation systems. <em>ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em> (pp. 1148–1160).</p>
</dd>
<dt class="label" id="id163"><span class="brackets">Chen et al., 2021</span></dt>
<dd><p>Chen, T., Zhang, Z., Liu, S., Chang, S., &amp; Wang, Z. (2021). Robust overfitting may be mitigated by properly learned smoothening. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">Chen et al., 2020</span></dt>
<dd><p>Chen, T., Kornblith, S., Norouzi, M., &amp; Hinton, G. (2020). A simple framework for contrastive learning of visual representations. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id401"><span class="brackets">Chen et al., 2023</span></dt>
<dd><p>Chen, W., Song, D., &amp; Li, B. (2023). Trojdiff: trojan attacks on diffusion models with diverse targets. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id104"><span class="brackets">Chen et al., 2017b</span></dt>
<dd><p>Chen, X., Liu, C., Li, B., Lu, K., &amp; Song, D. (2017). <em>Targeted backdoor attacks on deep learning systems using data poisoning</em>.</p>
</dd>
<dt class="label" id="id426"><span class="brackets">Chen et al., 2024</span></dt>
<dd><p>Chen, Y., Ma, X., Zou, D., &amp; Jiang, Y.-G. (2024). Extracting training data from unconditional diffusion models. <em>arXiv preprint arXiv:2406.12752</em>.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">Cheng et al., 2019</span></dt>
<dd><p>Cheng, M., Le, T., Chen, P.-Y., Zhang, H., Yi, J., &amp; Hsieh, C.-J. (2019). Query-efficient hard-label black-box attack: an optimization-based approach. <em>International Conference on Learning Representation</em>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">Chiang et al., 2023</span></dt>
<dd><p>Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., … others. (2023). Vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality. <em>See https://vicuna. lmsys. org (accessed 14 April 2023)</em>.</p>
</dd>
<dt class="label" id="id402"><span class="brackets">Chou et al., 2023</span></dt>
<dd><p>Chou, S.-Y., Chen, P.-Y., &amp; Ho, T.-Y. (2023). How to backdoor diffusion models? <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">Chowdhery et al., 2023</span></dt>
<dd><p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., … others. (2023). Palm: scaling language modeling with pathways. <em>Journal of Machine Learning Research</em>, <em>24</em>(240), 1–113.</p>
</dd>
<dt class="label" id="id313"><span class="brackets">Christiano et al., 2017</span></dt>
<dd><p>Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id165"><span class="brackets">Clevert et al., 2016</span></dt>
<dd><p>Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2016). Fast and accurate deep network learning by exponential linear units (elus). <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">Cordts et al., 2016</span></dt>
<dd><p>Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., … Schiele, B. (2016). The cityscapes dataset for semantic urban scene understanding. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">Croce &amp; Hein, 2020a</span></dt>
<dd><p>Croce, F., &amp; Hein, M. (2020). Minimally distorted adversarial examples with a fast adaptive boundary attack. <em>International Conference on Machine Learning</em> (pp. 2196–2205).</p>
</dd>
<dt class="label" id="id72"><span class="brackets">Croce &amp; Hein, 2020b</span></dt>
<dd><p>Croce, F., &amp; Hein, M. (2020). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. <em>International Conference on Machine Learning</em> (pp. 2206–2216).</p>
</dd>
<dt class="label" id="id429"><span class="brackets">Crowson, 2022</span></dt>
<dd><p>Crowson, K. (2022). <em>K-Diffusion</em>.</p>
</dd>
<dt class="label" id="id369"><span class="brackets">Dai et al., 2023</span></dt>
<dd><p>Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., … Hoi, S. (2023). <em>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</em>.</p>
</dd>
<dt class="label" id="id167"><span class="brackets">DarvishRouhani et al., 2019</span></dt>
<dd><p>Darvish Rouhani, B., Chen, H., &amp; Koushanfar, F. (2019). Deepsigns: an end-to-end watermarking framework for ownership protection of deep neural networks. <em>International Conference on Architectural Support for Programming Languages and Operating Systems</em>.</p>
</dd>
<dt class="label" id="id168"><span class="brackets">Das et al., 2017</span></dt>
<dd><p>Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen, L., Kounavis, M. E., &amp; Chau, D. H. (2017). <em>Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression</em>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">Deng et al., 2009</span></dt>
<dd><p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: a large-scale hierarchical image database. <em>IEEE Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">Devlin et al., 2018</span></dt>
<dd><p>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2018). Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>.</p>
</dd>
<dt class="label" id="id169"><span class="brackets">Ding et al., 2019</span></dt>
<dd><p>Ding, G. W., Sharma, Y., Lui, K. Y. C., &amp; Huang, R. (2019). Mma training: direct input space margin maximization through adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id294"><span class="brackets">Doan et al., 2023</span></dt>
<dd><p>Doan, K. D., Lao, Y., Yang, P., &amp; Li, P. (2023). Defending backdoor attacks on vision transformer via patch processing. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id170"><span class="brackets">Dong et al., 2020</span></dt>
<dd><p>Dong, Y., Deng, Z., Pang, T., Zhu, J., &amp; Su, H. (2020). Adversarial distributional training for robust deep learning. <em>Advances in Neural Information Processing Systems</em> (pp. 8270–8283).</p>
</dd>
<dt class="label" id="id67"><span class="brackets">Dong et al., 2018</span></dt>
<dd><p>Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., &amp; Li, J. (2018). Boosting adversarial attacks with momentum. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 9185–9193).</p>
</dd>
<dt class="label" id="id37"><span class="brackets">Dosovitskiy et al., 2021</span></dt>
<dd><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … others. (2021). An image is worth 16x16 words: transformers for image recognition at scale. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id430"><span class="brackets">Duan et al., 2023</span></dt>
<dd><p>Duan, J., Kong, F., Wang, S., Shi, X., &amp; Xu, K. (2023). Are diffusion models vulnerable to membership inference attacks? <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id86"><span class="brackets">Duan et al., 2020</span></dt>
<dd><p>Duan, R., Ma, X., Wang, Y., Bailey, J., Qin, A. K., &amp; Yang, Y. (2020). Adversarial camouflage: hiding physical-world attacks with natural styles. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 1000–1008).</p>
</dd>
<dt class="label" id="id171"><span class="brackets">Dwork et al., 2006</span></dt>
<dd><p>Dwork, C., McSherry, F., Nissim, K., &amp; Smith, A. (2006). Calibrating noise to sensitivity in private data analysis. <em>Theory of Cryptography Conference</em>.</p>
</dd>
<dt class="label" id="id316"><span class="brackets">Ebrahimi et al., 2018</span></dt>
<dd><p>Ebrahimi, J., Rao, A., Lowd, D., &amp; Dou, D. (2018). Hotflip: white-box adversarial examples for text classification. <em>Annual Meeting of the Association for Computational Linguistics</em> (pp. 31–36).</p>
</dd>
<dt class="label" id="id34"><span class="brackets">Elman, 1990</span></dt>
<dd><p>Elman, J. L. (1990). Finding structure in time. <em>Cognitive Science</em>, <em>14</em>, 179–211.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">Eykholt et al., 2018</span></dt>
<dd><p>Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., … Song, D. (2018). Robust physical-world attacks on deep learning visual classification. <em>IEEE Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id173"><span class="brackets">Feinman et al., 2017</span></dt>
<dd><p>Feinman, R., Curtin, R. R., Shintre, S., &amp; Gardner, A. B. (2017). <em>Detecting adversarial samples from artifacts</em>.</p>
</dd>
<dt class="label" id="id100"><span class="brackets">Feng et al., 2019</span></dt>
<dd><p>Feng, J., Cai, Q.-Z., &amp; Zhou, Z.-H. (2019). Learning to confuse: generating training time adversarial data with auto-encoder. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id174"><span class="brackets">Fredrikson et al., 2015</span></dt>
<dd><p>Fredrikson, M., Jha, S., &amp; Ristenpart, T. (2015). Model inversion attacks that exploit confidence information and basic countermeasures. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 1322–1333).</p>
</dd>
<dt class="label" id="id175"><span class="brackets">Frosst et al., 2019</span></dt>
<dd><p>Frosst, N., Papernot, N., &amp; Hinton, G. (2019). Analyzing and improving representations with the soft nearest neighbor loss. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id270"><span class="brackets">Fu et al., 2022</span></dt>
<dd><p>Fu, Y., Zhang, S., Wu, S., Wan, C., &amp; Lin, Y. (2022). Patch-fool: are vision transformers always robust against adversarial perturbations? <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id176"><span class="brackets">Fung et al., 2018</span></dt>
<dd><p>Fung, C., Yoon, C. J., &amp; Beschastnikh, I. (2018). <em>Mitigating sybils in federated learning poisoning</em>.</p>
</dd>
<dt class="label" id="id343"><span class="brackets">Gailly &amp; Adler, 2004</span></dt>
<dd><p>Gailly, J.-l., &amp; Adler, M. (2004). Zlib compression library.</p>
</dd>
<dt class="label" id="id177"><span class="brackets">Gal &amp; Ghahramani, 2016</span></dt>
<dd><p>Gal, Y., &amp; Ghahramani, Z. (2016). A theoretically grounded application of dropout in recurrent neural networks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id390"><span class="brackets">Gan et al., 2020</span></dt>
<dd><p>Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., &amp; Liu, J. (2020). Large-scale adversarial training for vision-and-language representation learning. <em>Advances in Neural Information Processing Systems</em> (pp. 6616–6628).</p>
</dd>
<dt class="label" id="id98"><span class="brackets">Geiping et al., 2021</span></dt>
<dd><p>Geiping, J., Fowl, L. H., Huang, W. R., Czaja, W., Taylor, G., Moeller, M., &amp; Goldstein, T. (2021). Witches' brew: industrial scale data poisoning via gradient matching. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id178"><span class="brackets">Goldblum et al., 2020</span></dt>
<dd><p>Goldblum, M., Fowl, L., Feizi, S., &amp; Goldstein, T. (2020). Adversarially robust distillation. <em>AAAI Conference on Artificial Intelligence</em> (pp. 3996–4003).</p>
</dd>
<dt class="label" id="id382"><span class="brackets">Gong et al., 2023</span></dt>
<dd><p>Gong, Y., Ran, D., Liu, J., Wang, C., Cong, T., Wang, A., … Wang, X. (2023). Figstep: jailbreaking large vision-language models via typographic visual prompts. <em>arXiv:2311.05608</em>.</p>
</dd>
<dt class="label" id="id179"><span class="brackets">Gong et al., 2017</span></dt>
<dd><p>Gong, Z., Wang, W., &amp; Ku, W.-S. (2017). <em>Adversarial and clean data are not twins</em>.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">Goodfellow et al., 2015</span></dt>
<dd><p>Goodfellow, I. J., Shlens, J., &amp; Szegedy, C. (2015). Explaining and harnessing adversarial examples. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id180"><span class="brackets">Gowal et al., 2021</span></dt>
<dd><p>Gowal, S., Rebuffi, S.-A., Wiles, O., Stimberg, F., Calian, D. A., &amp; Mann, T. A. (2021). Improving robustness using generated data. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id349"><span class="brackets">Goyal et al., 2020</span></dt>
<dd><p>Goyal, S., Choudhury, A. R., Raje, S. M., Chakaravarthy, V. T., Sabharwal, Y., &amp; Verma, A. (2020). Power-bert: accelerating bert inference via progressive word-vector elimination. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id381"><span class="brackets">Greshake et al., 2023</span></dt>
<dd><p>Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., &amp; Fritz, M. (2023). More than you've asked for: a comprehensive analysis of novel prompt injection threats to application-integrated large language models. <em>arXiv e-prints</em>, pp. arXiv–2302.</p>
</dd>
<dt class="label" id="id181"><span class="brackets">Gretton et al., 2012</span></dt>
<dd><p>Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., &amp; Smola, A. (2012). A kernel two-sample test. <em>The Journal of Machine Learning Research</em>, <em>13</em>(1), 723–773.</p>
</dd>
<dt class="label" id="id182"><span class="brackets">Grosse et al., 2017</span></dt>
<dd><p>Grosse, K., Manoharan, P., Papernot, N., Backes, M., &amp; McDaniel, P. (2017). <em>On the (statistical) detection of adversarial examples</em>.</p>
</dd>
<dt class="label" id="id269"><span class="brackets">Gu et al., 2022</span></dt>
<dd><p>Gu, J., Tresp, V., &amp; Qin, Y. (2022). Are vision transformers robust to patch perturbations? <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets">Gu et al., 2017</span></dt>
<dd><p>Gu, T., Dolan-Gavitt, B., &amp; Garg, S. (2017). <em>Badnets: Identifying vulnerabilities in the machine learning model supply chain</em>.</p>
</dd>
<dt class="label" id="id423"><span class="brackets">Gu et al., 2023</span></dt>
<dd><p>Gu, X., Du, C., Pang, T., Li, C., Lin, M., &amp; Wang, Y. (2023). On memorization in diffusion models. <em>arXiv preprint arXiv:2310.02664</em>.</p>
</dd>
<dt class="label" id="id353"><span class="brackets">Guan et al., 2022</span></dt>
<dd><p>Guan, Y., Li, Z., Leng, J., Lin, Z., &amp; Guo, M. (2022). Transkimmer: transformer learns to layer-wise skim. <em>Annual Meeting of the Association for Computational Linguistics</em> (pp. 7275–7286).</p>
</dd>
<dt class="label" id="id415"><span class="brackets">Guan et al., 2024</span></dt>
<dd><p>Guan, Z., Hu, M., Li, S., &amp; Vullikanti, A. (2024). Ufid: a unified framework for input-level backdoor detection on diffusion models. <em>arXiv preprint arXiv:2404.01101</em>.</p>
</dd>
<dt class="label" id="id432"><span class="brackets">Guo et al., 2017</span></dt>
<dd><p>Guo, C., Pleiss, G., Sun, Y., &amp; Weinberger, K. Q. (2017). On calibration of modern neural networks. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id373"><span class="brackets">Guo et al., 2023</span></dt>
<dd><p>Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., &amp; Hoi, S. (2023). From images to textual prompts: zero-shot visual question answering with frozen large language models. <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 10867–10877).</p>
</dd>
<dt class="label" id="id183"><span class="brackets">Guo et al., 2019</span></dt>
<dd><p>Guo, W., Wang, L., Xing, X., Du, M., &amp; Song, D. (2019). <em>Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems</em>.</p>
</dd>
<dt class="label" id="id184"><span class="brackets">Gupta &amp; Rahtu, 2019</span></dt>
<dd><p>Gupta, P., &amp; Rahtu, E. (2019). Ciidefence: defeating adversarial attacks by fusing class-specific image inpainting and image denoising. <em>IEEE International Conference on Computer Vision</em> (pp. 6708–6717).</p>
</dd>
<dt class="label" id="id185"><span class="brackets">Hampel, 1974</span></dt>
<dd><p>Hampel, F. R. (1974). The influence curve and its role in robust estimation. <em>Journal of the American Statistical Association</em>, <em>69</em>(346), 383–393.</p>
</dd>
<dt class="label" id="id312"><span class="brackets">Hao et al., 2024</span></dt>
<dd><p>Hao, Y., Yang, W., &amp; Lin, Y. (2024). Exploring backdoor vulnerabilities of chat models. <em>arXiv preprint arXiv:2404.02406</em>.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">He et al., 2022a</span></dt>
<dd><p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2022). Masked autoencoders are scalable vision learners. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">He et al., 2016</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id357"><span class="brackets">He et al., 2022b</span></dt>
<dd><p>He, X., Xu, Q., Lyu, L., Wu, F., &amp; Wang, C. (2022). Protecting intellectual property of language generation apis with lexical watermark. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id121"><span class="brackets">He et al., 2019</span></dt>
<dd><p>He, Z., Zhang, T., &amp; Lee, R. B. (2019). Model inversion attacks against collaborative inference. <em>Annual Computer Security Applications Conference</em>.</p>
</dd>
<dt class="label" id="id186"><span class="brackets">Hendrycks &amp; Gimpel, 2016a</span></dt>
<dd><p>Hendrycks, D., &amp; Gimpel, K. (2016). <em>Early methods for detecting adversarial images</em>.</p>
</dd>
<dt class="label" id="id187"><span class="brackets">Hendrycks &amp; Gimpel, 2016b</span></dt>
<dd><p>Hendrycks, D., &amp; Gimpel, K. (2016). <em>Gaussian error linear units (gelus)</em>.</p>
</dd>
<dt class="label" id="id419"><span class="brackets">Hintersdorf et al., 2024</span></dt>
<dd><p>Hintersdorf, D., Struppek, L., Kersting, K., Dziedzic, A., &amp; Boenisch, F. (2024). Finding nemo: localizing neurons responsible for memorization in diffusion models. <em>arXiv preprint arXiv:2406.02366</em>.</p>
</dd>
<dt class="label" id="id129"><span class="brackets">Hinton et al., 2015</span></dt>
<dd><p>Hinton, G., Vinyals, O., &amp; Dean, J. (2015). Distilling the knowledge in a neural network. <em>arXiv preprint arXiv:1503.02531</em>.</p>
</dd>
<dt class="label" id="id188"><span class="brackets">Ho et al., 2020</span></dt>
<dd><p>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id408"><span class="brackets">Houle, 2017</span></dt>
<dd><p>Houle, M. E. (2017). Local intrinsic dimensionality I: an extreme-value-theoretic foundation for similarity applications. <em>International Conference on Similarity Search and Applications</em>.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">Hu et al., 2022</span></dt>
<dd><p>Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., … Chen, W. (2022). LoRA: low-rank adaptation of large language models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id189"><span class="brackets">Hu et al., 2019</span></dt>
<dd><p>Hu, S., Yu, T., Guo, C., Chao, W.-L., &amp; Weinberger, K. Q. (2019). A new defense against adversarial images: turning a weakness into a strength. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id278"><span class="brackets">Hua et al., 2024</span></dt>
<dd><p>Hua, A., Gu, J., Xue, Z., Carlini, N., Wong, E., &amp; Qin, Y. (2024). Initialization matters for adversarial transfer learning. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 24831–24840).</p>
</dd>
<dt class="label" id="id308"><span class="brackets">Huang et al., 2023a</span></dt>
<dd><p>Huang, B., Wang, Z., Yang, J., Ai, J., Zou, Q., Wang, Q., &amp; Ye, D. (2023). Implicit identity driven deepfake face swapping detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id190"><span class="brackets">Huang et al., 2023b</span></dt>
<dd><p>Huang, H., Ma, X., Erfani, S., &amp; Bailey, J. (2023). Distilling cognitive backdoor patterns within an image. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id97"><span class="brackets">Huang et al., 2020</span></dt>
<dd><p>Huang, W. R., Geiping, J., Fowl, L., Taylor, G., &amp; Goldstein, T. (2020). Metapoison: practical general-purpose clean-label data poisoning. <em>Advances in Neural Information Processing Systems</em> (pp. 12080–12091).</p>
</dd>
<dt class="label" id="id77"><span class="brackets">Ilyas et al., 2018</span></dt>
<dd><p>Ilyas, A., Engstrom, L., Athalye, A., &amp; Lin, J. (2018). Black-box adversarial attacks with limited queries and information. <em>International Conference on Machine Learning</em> (pp. 2137–2146).</p>
</dd>
<dt class="label" id="id342"><span class="brackets">Ishihara, 2023</span></dt>
<dd><p>Ishihara, S. (2023). Training data extraction from pre-trained language models: a survey. <em>arXiv preprint arXiv:2305.16157</em>.</p>
</dd>
<dt class="label" id="id191"><span class="brackets">Izmailov et al., 2018</span></dt>
<dd><p>Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., &amp; Wilson, A. G. (2018). Averaging weights leads to wider optima and better generalization. <em>Conference on Uncertainty in Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id193"><span class="brackets">Jia et al., 2021</span></dt>
<dd><p>Jia, H., Choquette-Choo, C. A., Chandrasekaran, V., &amp; Papernot, N. (2021). Entangled watermarks as a defense against model extraction. <em>USENIX Security Symposium</em>.</p>
</dd>
<dt class="label" id="id396"><span class="brackets">Jia et al., 2022a</span></dt>
<dd><p>Jia, J., Liu, Y., &amp; Gong, N. Z. (2022). Badencoder: backdoor attacks to pre-trained encoders in self-supervised learning. <em>IEEE Symposium on Security and Privacy</em>.</p>
</dd>
<dt class="label" id="id61"><span class="brackets">Jia et al., 2022b</span></dt>
<dd><p>Jia, M., Tang, L., Chen, B.-C., Cardie, C., Belongie, S., Hariharan, B., &amp; Lim, S.-N. (2022). Visual prompt tuning. <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id192"><span class="brackets">Jia et al., 2019</span></dt>
<dd><p>Jia, X., Wei, X., Cao, X., &amp; Foroosh, H. (2019). Comdefend: an efficient image compression model to defend adversarial examples. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 6084–6092).</p>
</dd>
<dt class="label" id="id327"><span class="brackets">Jiang et al., 2023</span></dt>
<dd><p>Jiang, Y., Chan, C., Chen, M., &amp; Wang, W. (2023). Lion: adversarial distillation of proprietary large language models. <em>Conference on Empirical Methods in Natural Language Processing</em> (pp. 3134–3154).</p>
</dd>
<dt class="label" id="id194"><span class="brackets">Jin et al., 2019</span></dt>
<dd><p>Jin, G., Shen, S., Zhang, D., Dai, F., &amp; Zhang, Y. (2019). Ape-gan: adversarial perturbation elimination with gan. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 3842–3846).</p>
</dd>
<dt class="label" id="id19"><span class="brackets">Kang et al., 2023</span></dt>
<dd><p>Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., &amp; Park, T. (2023). Scaling up gans for text-to-image synthesis. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">Kearns &amp; Li, 1993</span></dt>
<dd><p>Kearns, M., &amp; Li, M. (1993). Learning in the presence of malicious errors. <em>SIAM Journal on Computing</em>, <em>22</em>(4), 807–837.</p>
</dd>
<dt class="label" id="id350"><span class="brackets">Kim &amp; Cho, 2021</span></dt>
<dd><p>Kim, G., &amp; Cho, K. (2021). Length-adaptive transformer: train once with length drop, use anytime with search. <em>Joint Conference of Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing</em>.</p>
</dd>
<dt class="label" id="id352"><span class="brackets">Kim et al., 2022</span></dt>
<dd><p>Kim, S., Shen, S., Thorsley, D., Gholami, A., Kwon, W., Hassoun, J., &amp; Keutzer, K. (2022). Learned token pruning for transformers. <em>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (pp. 784–794).</p>
</dd>
<dt class="label" id="id358"><span class="brackets">Kirchenbauer et al., 2023</span></dt>
<dd><p>Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., &amp; Goldstein, T. (2023). A watermark for large language models. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id102"><span class="brackets">Koh et al., 2022</span></dt>
<dd><p>Koh, P. W., Steinhardt, J., &amp; Liang, P. (2022). Stronger data poisoning attacks break data sanitization defenses. <em>Machine Learning</em>, <em>111</em>(1), 1–47.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">Kruger et al., 2004</span></dt>
<dd><p>Kruger, L. E., Wohler, C., Wurz-Wessel, A., &amp; Stein, F. (2004). In-factory calibration of multiocular camera systems. <em>Optical Metrology in Production Engineering</em>.</p>
</dd>
<dt class="label" id="id88"><span class="brackets">Kumar et al., 2020</span></dt>
<dd><p>Kumar, R. S. S., Nyström, M., Lambert, J., Marshall, A., Goertzel, M., Comissoneru, A., … Xia, S. (2020). Adversarial machine learning-industry perspectives. <em>IEEE Security and Privacy Workshops</em> (pp. 69–75).</p>
</dd>
<dt class="label" id="id195"><span class="brackets">Kurakin et al., 2016</span></dt>
<dd><p>Kurakin, A., Goodfellow, I., &amp; Bengio, S. (2016). <em>Adversarial machine learning at scale</em>.</p>
</dd>
<dt class="label" id="id65"><span class="brackets">Kurakin et al., 2018</span></dt>
<dd><p>Kurakin, A., Goodfellow, I. J., &amp; Bengio, S. (2018). Adversarial examples in the physical world. <em>Artificial Intelligence Safety and Security</em> (pp. 99–112). Chapman and Hall/CRC.</p>
</dd>
<dt class="label" id="id196"><span class="brackets">LeMerrer et al., 2020</span></dt>
<dd><p>Le Merrer, E., Perez, P., &amp; Trédan, G. (2020). Adversarial frontier stitching for remote neural network watermarking. <em>Neural Computing and Applications</em>, <em>32</em>(13), 9233–9244.</p>
</dd>
<dt class="label" id="id333"><span class="brackets">Lee et al., 2022</span></dt>
<dd><p>Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., &amp; Carlini, N. (2022). Deduplicating training data makes language models better. <em>Annual Meeting of the Association for Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id197"><span class="brackets">Lee et al., 2018</span></dt>
<dd><p>Lee, K., Lee, K., Lee, H., &amp; Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id326"><span class="brackets">Li et al., 2024a</span></dt>
<dd><p>Li, H., Chen, Y., Zheng, Z., Hu, Q., Chan, C., Liu, H., &amp; Song, Y. (2024). Backdoor removal for generative large language models. <em>arXiv preprint arXiv:2405.07667</em>.</p>
</dd>
<dt class="label" id="id371"><span class="brackets">Li et al., 2023a</span></dt>
<dd><p>Li, J., Li, D., Savarese, S., &amp; Hoi, S. (2023). Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. <em>International conference on machine learning</em> (pp. 19730–19742).</p>
</dd>
<dt class="label" id="id370"><span class="brackets">Li et al., 2022</span></dt>
<dd><p>Li, J., Li, D., Xiong, C., &amp; Hoi, S. (2022). Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation. <em>International conference on machine learning</em> (pp. 12888–12900).</p>
</dd>
<dt class="label" id="id367"><span class="brackets">Li et al., 2021a</span></dt>
<dd><p>Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., &amp; Hoi, S. C. H. (2021). Align before fuse: vision and language representation learning with momentum distillation. <em>Advances in neural information processing systems</em>, <em>34</em>, 9694–9705.</p>
</dd>
<dt class="label" id="id306"><span class="brackets">Li et al., 2020a</span></dt>
<dd><p>Li, L., Bao, J., Zhang, T., Yang, H., Chen, D., Wen, F., &amp; Guo, B. (2020). Face x-ray for more general face forgery detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id317"><span class="brackets">Li et al., 2020b</span></dt>
<dd><p>Li, L., Ma, R., Guo, Q., Xue, X., &amp; Qiu, X. (2020). Bert-attack: adversarial attack against bert using bert. <em>Conference on Empirical Methods in Natural Language Processing</em> (pp. 6193–6202).</p>
</dd>
<dt class="label" id="id301"><span class="brackets">Li et al., 2024b</span></dt>
<dd><p>Li, Q., Wang, W., Xu, C., Sun, Z., &amp; Yang, M.-H. (2024). Learning disentangled representation for one-shot progressive face swapping. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>
</dd>
<dt class="label" id="id199"><span class="brackets">Li et al., 2020c</span></dt>
<dd><p>Li, S., Cheng, Y., Wang, W., Liu, Y., &amp; Chen, T. (2020). Learning to detect malicious clients for robust federated learning. <em>arXiv preprint arXiv:2002.00211</em>.</p>
</dd>
<dt class="label" id="id417"><span class="brackets">Li et al., 2024c</span></dt>
<dd><p>Li, W., Chen, P.-Y., Liu, S., &amp; Wang, R. (2024). Psbd: prediction shift uncertainty unlocks backdoor detection. <em>arXiv preprint arXiv:2406.05826</em>.</p>
</dd>
<dt class="label" id="id200"><span class="brackets">Li et al., 2021b</span></dt>
<dd><p>Li, Y., Yang, Z., Wang, Y., &amp; Xu, C. (2021). Neural architecture dilation for adversarial robustness. <em>Advances in Neural Information Processing Systems</em> (pp. 29578–29589).</p>
</dd>
<dt class="label" id="id139"><span class="brackets">Li et al., 2021c</span></dt>
<dd><p>Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., &amp; Ma, X. (2021). Anti-backdoor learning: training clean models on poisoned data. <em>Advances in Neural Information Processing Systems</em> (pp. 14900–14912).</p>
</dd>
<dt class="label" id="id346"><span class="brackets">Li et al., 2021d</span></dt>
<dd><p>Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., &amp; Ma, X. (2021). Anti-backdoor learning: training clean models on poisoned data. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id201"><span class="brackets">Li et al., 2023b</span></dt>
<dd><p>Li, Y., Lyu, X., Ma, X., Koren, N., Lyu, L., Li, B., &amp; Jiang, Y.-G. (2023). Reconstructive neuron pruning for backdoor defense. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id289"><span class="brackets">Li et al., 2024d</span></dt>
<dd><p>Li, Y., Ma, X., He, J., Huang, H., &amp; Jiang, Y.-G. (2024). Multi-trigger backdoor attacks: more triggers, more threats. <em>arXiv preprint arXiv:2401.15295</em>.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">Li et al., 2021e</span></dt>
<dd><p>Li, Y., Li, Y., Wu, B., Li, L., He, R., &amp; Lyu, S. (2021). Invisible backdoor attack with sample-specific triggers. <em>IEEE International Conference on Computer Vision</em> (pp. 16463–16472).</p>
</dd>
<dt class="label" id="id328"><span class="brackets">Li et al., 2024e</span></dt>
<dd><p>Li, Z., Wang, C., Ma, P., Liu, C., Wang, S., Wu, D., … Liu, Y. (2024). On extracting specialized code abilities from large language models: a feasibility study. <em>IEEE/ACM International Conference on Software Engineering</em>.</p>
</dd>
<dt class="label" id="id399"><span class="brackets">Liang et al., 2024</span></dt>
<dd><p>Liang, S., Zhu, M., Liu, A., Wu, B., Cao, X., &amp; Chang, E.-C. (2024). Badclip: dual-embedding guided backdoor attack on multimodal contrastive learning. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id202"><span class="brackets">Liao et al., 2018</span></dt>
<dd><p>Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., &amp; Zhu, J. (2018). Defense against adversarial attacks using high-level representation guided denoiser. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 1778–1787).</p>
</dd>
<dt class="label" id="id58"><span class="brackets">Lin et al., 2014</span></dt>
<dd><p>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., … Zitnick, C. L. (2014). Microsoft coco: common objects in context. <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">Liu et al., 2023</span></dt>
<dd><p>Liu, H., Li, C., Wu, Q., &amp; Lee, Y. J. (2023). Visual instruction tuning. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id413"><span class="brackets">Liu et al., 2024</span></dt>
<dd><p>Liu, H., Reiter, M. K., &amp; Gong, N. Z. (2024). Mudjacking: patching backdoor vulnerabilities in foundation models. <em>arXiv preprint arXiv:2402.14977</em>.</p>
</dd>
<dt class="label" id="id203"><span class="brackets">Liu et al., 2018a</span></dt>
<dd><p>Liu, K., Dolan-Gavitt, B., &amp; Garg, S. (2018). Fine-pruning: defending against backdooring attacks on deep neural networks. <em>International Symposium on Research in Attacks, Intrusions, and Defenses</em> (pp. 273–294).</p>
</dd>
<dt class="label" id="id324"><span class="brackets">Liu et al., 2020</span></dt>
<dd><p>Liu, X., Cheng, H., He, P., Chen, W., Wang, Y., Poon, H., &amp; Gao, J. (2020). Adversarial training for large neural language models. <em>arXiv preprint arXiv:2004.08994</em>.</p>
</dd>
<dt class="label" id="id80"><span class="brackets">Liu et al., 2017</span></dt>
<dd><p>Liu, Y., Chen, X., Liu, C., &amp; Song, D. (2017). <em>Delving into transferable adversarial examples and black-box attacks</em>.</p>
</dd>
<dt class="label" id="id107"><span class="brackets">Liu et al., 2018b</span></dt>
<dd><p>Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., &amp; Zhang, X. (2018). Trojaning attack on neural networks. <em>Network and Distributed Systems Security Symposium</em>.</p>
</dd>
<dt class="label" id="id204"><span class="brackets">Lorenz et al., 2022</span></dt>
<dd><p>Lorenz, P., Keuper, M., &amp; Keuper, J. (2022). Unfolding local growth rate estimates for (almost) perfect adversarial detection. <em>International Conference on Computer Vision Theory and Applications</em>.</p>
</dd>
<dt class="label" id="id375"><span class="brackets">Lu et al., 2023</span></dt>
<dd><p>Lu, D., Wang, Z., Wang, T., Guan, W., Gao, H., &amp; Zheng, F. (2023). Set-level guidance attack: boosting adversarial transferability of vision-language pre-training models. <em>IEEE/CVF International Conference on Computer Vision</em> (pp. 102–111).</p>
</dd>
<dt class="label" id="id53"><span class="brackets">Lu et al., 2022</span></dt>
<dd><p>Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., … Kalyan, A. (2022). Learn to explain: multimodal reasoning via thought chains for science question answering. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id205"><span class="brackets">Lukas et al., 2021</span></dt>
<dd><p>Lukas, N., Zhang, Y., &amp; Kerschbaum, F. (2021). <em>Deep neural network fingerprinting by conferrable adversarial examples</em>.</p>
</dd>
<dt class="label" id="id379"><span class="brackets">Luo et al., 2024</span></dt>
<dd><p>Luo, H., Gu, J., Liu, F., &amp; Torr, P. (2024). An image is worth 1000 lies: adversarial transferability across prompts on vision-language models. <em>arXiv:2403.09766</em>.</p>
</dd>
<dt class="label" id="id283"><span class="brackets">Lv et al., 2021</span></dt>
<dd><p>Lv, P., Ma, H., Zhou, J., Liang, R., Chen, K., Zhang, S., &amp; Yang, Y. (2021). Dbia: data-free backdoor injection attack against transformer networks. <em>arXiv preprint arXiv:2111.11870</em>.</p>
</dd>
<dt class="label" id="id290"><span class="brackets">Ma et al., 2023</span></dt>
<dd><p>Ma, H., Qiu, H., Gao, Y., Zhang, Z., Abuadbba, A., Xue, M., … Abbott, D. (2023). Quantization backdoors to deep learning commercial frameworks. <em>IEEE Transactions on Dependable and Secure Computing</em>.</p>
</dd>
<dt class="label" id="id387"><span class="brackets">Ma et al., 2024</span></dt>
<dd><p>Ma, J., Cao, A., Xiao, Z., Zhang, J., Ye, C., &amp; Zhao, J. (2024). Jailbreaking prompt attack: a controllable adversarial attack against diffusion models. <em>arXiv:2404.02928</em>.</p>
</dd>
<dt class="label" id="id206"><span class="brackets">Ma et al., 2018</span></dt>
<dd><p>Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S., Schoenebeck, G., … Bailey, J. (2018). Characterizing adversarial subspaces using local intrinsic dimensionality. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">Madry et al., 2018</span></dt>
<dd><p>Madry, A., Makelov, A., Schmidt, L., Tsipras, D., &amp; Vladu, A. (2018). Towards deep learning models resistant to adversarial attacks. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id207"><span class="brackets">Mahalanobis, 1936</span></dt>
<dd><p>Mahalanobis, P. C. (1936). On the generalized distance in statistics. <em>Proceedings of the National Institute of Sciences</em>, <em>2</em>, 49–55.</p>
</dd>
<dt class="label" id="id123"><span class="brackets">Mahendran &amp; Vedaldi, 2015</span></dt>
<dd><p>Mahendran, A., &amp; Vedaldi, A. (2015). Understanding deep image representations by inverting them. <em>IEEE Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id124"><span class="brackets">Mahendran &amp; Vedaldi, 2016</span></dt>
<dd><p>Mahendran, A., &amp; Vedaldi, A. (2016). Visualizing deep convolutional neural networks using natural pre-images. <em>International Journal of Computer Vision</em>, <em>120</em>, 233–255.</p>
</dd>
<dt class="label" id="id93"><span class="brackets">Mahloujifar &amp; Mahmoody, 2017</span></dt>
<dd><p>Mahloujifar, S., &amp; Mahmoody, M. (2017). Blockwise p-tampering attacks on cryptographic primitives, extractors, and learners. <em>Theory of Cryptography Conference</em> (pp. 245–279).</p>
</dd>
<dt class="label" id="id94"><span class="brackets">Mahloujifar et al., 2019</span></dt>
<dd><p>Mahloujifar, S., Mahmoody, M., &amp; Mohammed, A. (2019). Universal multi-party poisoning attacks. <em>International Conference on Machine Learning</em> (pp. 4274–4283).</p>
</dd>
<dt class="label" id="id268"><span class="brackets">Mahmood et al., 2021</span></dt>
<dd><p>Mahmood, K., Mahmood, R., &amp; Van Dijk, M. (2021). On the robustness of vision transformers to adversarial examples. <em>IEEE International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id393"><span class="brackets">Mao et al., 2023</span></dt>
<dd><p>Mao, C., Geng, S., Yang, J., Wang, X., &amp; Vondrick, C. (2023). Understanding zero-shot adversarial robustness for large-scale models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id303"><span class="brackets">Masood et al., 2023</span></dt>
<dd><p>Masood, M., Nawaz, M., Malik, K. M., Javed, A., Irtaza, A., &amp; Malik, H. (2023). Deepfakes generation and detection: state-of-the-art, open challenges, countermeasures, and way forward. <em>Applied Intelligence</em>, <em>53</em>(4), 3974–4026.</p>
</dd>
<dt class="label" id="id359"><span class="brackets">Mattern et al., 2023</span></dt>
<dd><p>Mattern, J., Mireshghallah, F., Jin, Z., Schoelkopf, B., Sachan, M., &amp; Berg-Kirkpatrick, T. (2023). Membership inference attacks against language models via neighbourhood comparison. <em>Annual Meeting of The Association For Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">McMahan et al., 2017</span></dt>
<dd><p>McMahan, B., Moore, E., Ramage, D., Hampson, S., &amp; y Arcas, B. A. (2017). Communication-efficient learning of deep networks from decentralized data. <em>Artificial Intelligence and Statistics</em>.</p>
</dd>
<dt class="label" id="id132"><span class="brackets">Mei &amp; Zhu, 2015</span></dt>
<dd><p>Mei, S., &amp; Zhu, X. (2015). Using machine teaching to identify optimal training-set attacks on machine learners. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id208"><span class="brackets">Meng &amp; Chen, 2017</span></dt>
<dd><p>Meng, D., &amp; Chen, H. (2017). Magnet: a two-pronged defense against adversarial examples. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 135–147).</p>
</dd>
<dt class="label" id="id209"><span class="brackets">Metzen et al., 2017</span></dt>
<dd><p>Metzen, J. H., Genewein, T., Fischer, V., &amp; Bischoff, B. (2017). On detecting adversarial perturbations. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id210"><span class="brackets">Micikevicius et al., 2018</span></dt>
<dd><p>Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., … others. (2018). Mixed precision training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id362"><span class="brackets">Miyato et al., 2018</span></dt>
<dd><p>Miyato, T., Maeda, S.-i., Koyama, M., &amp; Ishii, S. (2018). Virtual adversarial training: a regularization method for supervised and semi-supervised learning. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>41</em>(8), 1979–1993.</p>
</dd>
<dt class="label" id="id416"><span class="brackets">Mo et al., 2024</span></dt>
<dd><p>Mo, Y., Huang, H., Li, M., Li, A., &amp; Wang, Y. (2024). Terd: a unified framework for safeguarding diffusion models against backdoors. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">Moosavi-Dezfooli et al., 2016</span></dt>
<dd><p>Moosavi-Dezfooli, S.-M., Fawzi, A., &amp; Frossard, P. (2016). Deepfool: a simple and accurate method to fool deep neural networks. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2574–2582).</p>
</dd>
<dt class="label" id="id127"><span class="brackets">Mordvintsev et al., 2015</span></dt>
<dd><p>Mordvintsev, A., Olah, C., &amp; Tyka, M. (2015). Inceptionism: going deeper into neural networks.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">Munoz-Gonzalez et al., 2019</span></dt>
<dd><p>Muñoz-González, L., Pfitzner, B., Russo, M., Carnerero-Cano, J., &amp; Lupu, E. C. (2019). <em>Poisoning attacks with generative adversarial nets</em>.</p>
</dd>
<dt class="label" id="id211"><span class="brackets">Nair &amp; Hinton, 2010</span></dt>
<dd><p>Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id272"><span class="brackets">Naseer et al., 2021</span></dt>
<dd><p>Naseer, M., Ranasinghe, K., Khan, S., Khan, F. S., &amp; Porikli, F. (2021). On improving adversarial transferability of vision transformers. <em>arXiv preprint arXiv:2106.04169</em>.</p>
</dd>
<dt class="label" id="id421"><span class="brackets">Naseh et al., 2023</span></dt>
<dd><p>Naseh, A., Roh, J., &amp; Houmansadr, A. (2023). Memory triggers: unveiling memorization in text-to-image generative models through word-level duplication. <em>arXiv preprint arXiv:2312.03692</em>.</p>
</dd>
<dt class="label" id="id340"><span class="brackets">Nasr et al., 2023</span></dt>
<dd><p>Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., … Lee, K. (2023). Scalable extraction of training data from (production) language models. <em>arXiv preprint arXiv:2311.17035</em>.</p>
</dd>
<dt class="label" id="id91"><span class="brackets">Nelson et al., 2008</span></dt>
<dd><p>Nelson, B., Barreno, M., Chi, F. J., Joseph, A. D., Rubinstein, B. I., Saini, U., … Xia, K. (2008). Exploiting machine learning to subvert your spam filter. <em>LEET</em>, <em>8</em>(1), 9.</p>
</dd>
<dt class="label" id="id125"><span class="brackets">Nguyen et al., 2017</span></dt>
<dd><p>Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A., &amp; Yosinski, J. (2017). Plug &amp; play generative networks: conditional iterative generation of images in latent space. <em>IEEE Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id126"><span class="brackets">Nguyen et al., 2016</span></dt>
<dd><p>Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., &amp; Clune, J. (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. <em>Advances in Neural Information Processing systems</em>, <em>29</em>.</p>
</dd>
<dt class="label" id="id105"><span class="brackets">Nguyen &amp; Tran, 2020</span></dt>
<dd><p>Nguyen, T. A., &amp; Tran, A. (2020). Input-aware dynamic backdoor attack. <em>Advances in Neural Information Processing Systems</em> (pp. 3454–3464).</p>
</dd>
<dt class="label" id="id280"><span class="brackets">Nie et al., 2022</span></dt>
<dd><p>Nie, W., Guo, B., Huang, Y., Xiao, C., Vahdat, A., &amp; Anandkumar, A. (2022). Diffusion models for adversarial purification. <em>International Conference on Machine Learning</em> (pp. 16805–16827).</p>
</dd>
<dt class="label" id="id300"><span class="brackets">Nirkin et al., 2019</span></dt>
<dd><p>Nirkin, Y., Keller, Y., &amp; Hassner, T. (2019). FSGAN: subject agnostic face swapping and reenactment. <em>IEEE International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id380"><span class="brackets">Noever &amp; Noever, 2021</span></dt>
<dd><p>Noever, D. A., &amp; Noever, S. E. M. (2021). Reading isn't believing: adversarial attacks on multi-modal neurons. <em>arXiv:2103.10480</em>.</p>
</dd>
<dt class="label" id="id120"><span class="brackets">Oh et al., 2019</span></dt>
<dd><p>Oh, S. J., Schiele, B., &amp; Fritz, M. (2019). Towards reverse-engineering black-box neural networks. <em>Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</em> (pp. 121–144). Springer.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">Ooms, 2024</span></dt>
<dd><p>Ooms, J. (2024). <em>cld3: Google's Compact Language Detector 3</em>. R package version 1.6.0. URL: <a class="reference external" href="https://docs.ropensci.org/cld3/ https://github.com/ropensci/cld3 https://ropensci.r-universe.dev/cld3">https://docs.ropensci.org/cld3/ https://github.com/ropensci/cld3 https://ropensci.r-universe.dev/cld3</a></p>
</dd>
<dt class="label" id="id276"><span class="brackets">Oord et al., 2018</span></dt>
<dd><p>Oord, A. v. d., Li, Y., &amp; Vinyals, O. (2018). <em>Representation learning with contrastive predictive coding</em>.</p>
</dd>
<dt class="label" id="id364"><span class="brackets">OpenAI, 2024</span></dt>
<dd><p>OpenAI (2024). <em>ChatGPT</em>. Accessed: 2024-07-23.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">Paperno et al., 2016</span></dt>
<dd><p>Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., … Fernández, R. (2016). <em>The LAMBADA dataset: Word prediction requiring a broad discourse context</em>.</p>
</dd>
<dt class="label" id="id81"><span class="brackets">Papernot et al., 2017</span></dt>
<dd><p>Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., &amp; Swami, A. (2017). Practical black-box attacks against machine learning. <em>ACM on Asia Conference on Computer and Communications Security</em> (pp. 506–519).</p>
</dd>
<dt class="label" id="id68"><span class="brackets">Papernot et al., 2016</span></dt>
<dd><p>Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., &amp; Swami, A. (2016). The limitations of deep learning in adversarial settings. <em>IEEE European Symposium on Security and Privacy</em> (pp. 372–387).</p>
</dd>
<dt class="label" id="id44"><span class="brackets">Papineni et al., 2002</span></dt>
<dd><p>Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. <em>Annual Meeting of the Association for Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">Peters et al., 2018</span></dt>
<dd><p>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. <em>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.</p>
</dd>
<dt class="label" id="id422"><span class="brackets">Pinto et al., 2024</span></dt>
<dd><p>Pinto, F., Rauschmayr, N., Tramèr, F., Torr, P., &amp; Tombari, F. (2024). Extracting training data from document-based vqa models. <em>arXiv preprint arXiv:2407.08707</em>.</p>
</dd>
<dt class="label" id="id213"><span class="brackets">Prakash et al., 2018</span></dt>
<dd><p>Prakash, A., Moran, N., Garber, S., DiLillo, A., &amp; Storer, J. (2018). Deflecting adversarial attacks with pixel deflection. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 8571–8580).</p>
</dd>
<dt class="label" id="id318"><span class="brackets">Pruthi et al., 2019</span></dt>
<dd><p>Pruthi, D., Dhingra, B., &amp; Lipton, Z. C. (2019). Combating adversarial misspellings with robust word recognition. <em>Annual Meeting of the Association for Computational Linguistics</em> (pp. 5582–5591).</p>
</dd>
<dt class="label" id="id384"><span class="brackets">Qi et al., 2023</span></dt>
<dd><p>Qi, X., Huang, K., Panda, A., Wang, M., &amp; Mittal, P. (2023). Visual adversarial examples jailbreak large language models. <em>arXiv:2306.13213</em>.</p>
</dd>
<dt class="label" id="id305"><span class="brackets">Qian et al., 2020</span></dt>
<dd><p>Qian, Y., Yin, G., Sheng, L., Chen, Z., &amp; Shao, J. (2020). Thinking in frequency: face forgery detection by mining frequency-aware clues. <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id215"><span class="brackets">Qin et al., 2019</span></dt>
<dd><p>Qin, C., Martens, J., Gowal, S., Krishnan, D., Dvijotham, K., Fawzi, A., … Kohli, P. (2019). Adversarial robustness through local linearization. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">Radford et al., 2021</span></dt>
<dd><p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … others. (2021). Learning transferable visual models from natural language supervision. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">Radford et al., 2018</span></dt>
<dd><p>Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., &amp; others. (2018). Improving language understanding by generative pre-training.</p>
</dd>
<dt class="label" id="id314"><span class="brackets">Rafailov et al., 2024</span></dt>
<dd><p>Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., &amp; Finn, C. (2024). Direct preference optimization: your language model is secretly a reward model. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id216"><span class="brackets">Ramachandran et al., 2017</span></dt>
<dd><p>Ramachandran, P., Zoph, B., &amp; Le, Q. V. (2017). <em>Searching for activation functions</em>.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">Ramesh et al., 2022</span></dt>
<dd><p>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. <em>arXiv preprint arXiv:2204.06125</em>.</p>
</dd>
<dt class="label" id="id218"><span class="brackets">Rebuffi et al., 2021a</span></dt>
<dd><p>Rebuffi, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., &amp; Mann, T. (2021). <em>Fixing data augmentation to improve adversarial robustness</em>.</p>
</dd>
<dt class="label" id="id217"><span class="brackets">Rebuffi et al., 2021b</span></dt>
<dd><p>Rebuffi, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., &amp; Mann, T. A. (2021). Data augmentation can improve robustness. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id219"><span class="brackets">Rice et al., 2020</span></dt>
<dd><p>Rice, L., Wong, E., &amp; Kolter, Z. (2020). Overfitting in adversarially robust deep learning. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id325"><span class="brackets">Robey et al., 2023</span></dt>
<dd><p>Robey, A., Wong, E., Hassani, H., &amp; Pappas, G. J. (2023). Smoothllm: defending large language models against jailbreaking attacks. <em>arXiv preprint arXiv:2310.03684</em>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">Rombach et al., 2022</span></dt>
<dd><p>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id220"><span class="brackets">Ronneberger et al., 2015</span></dt>
<dd><p>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-net: convolutional networks for biomedical image segmentation. <em>International Conference on Medical Image Computing and Computer Assisted Intervention</em> (pp. 234–241).</p>
</dd>
<dt class="label" id="id221"><span class="brackets">Roth et al., 2019</span></dt>
<dd><p>Roth, K., Kilcher, Y., &amp; Hofmann, T. (2019). The odds are odd: a statistical test for detecting adversarial examples. <em>International Conference on Machine Learning</em> (pp. 5498–5507).</p>
</dd>
<dt class="label" id="id134"><span class="brackets">Saha et al., 2020</span></dt>
<dd><p>Saha, A., Subramanya, A., &amp; Pirsiavash, H. (2020). Hidden trigger backdoor attacks. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id330"><span class="brackets">Sakaguchi et al., 2017</span></dt>
<dd><p>Sakaguchi, K., Duh, K., Post, M., &amp; Van Durme, B. (2017). Robsut wrod reocginiton via semi-character recurrent neural network. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id222"><span class="brackets">Samangouei et al., 2018</span></dt>
<dd><p>Samangouei, P., Kabkab, M., &amp; Chellappa, R. (2018). Defense-gan: protecting classifiers against adversarial attacks using generative models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id394"><span class="brackets">Schlarmann et al., 2024</span></dt>
<dd><p>Schlarmann, C., Singh, N. D., Croce, F., &amp; Hein, M. (2024). Robust clip: unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id407"><span class="brackets">Schubert et al., 2014</span></dt>
<dd><p>Schubert, E., Zimek, A., &amp; Kriegel, H.-P. (2014). Local outlier detection reconsidered: a generalized view on locality with applications to spatial, video, and network outlier detection. <em>Data mining and knowledge discovery</em>, <em>28</em>, 190–237.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">Schuhmann et al., 2022</span></dt>
<dd><p>Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C. W., Wightman, R., Cherti, M., … Jitsev, J. (2022). LAION-5b: an open large-scale dataset for training next generation image-text models. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id315"><span class="brackets">Schulman et al., 2017</span></dt>
<dd><p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. <em>arXiv preprint arXiv:1707.06347</em>.</p>
</dd>
<dt class="label" id="id292"><span class="brackets">Selvaraju et al., 2017</span></dt>
<dd><p>Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., &amp; Batra, D. (2017). Grad-cam: visual explanations from deep networks via gradient-based localization. <em>IEEE International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id411"><span class="brackets">Sennrich et al., 2016</span></dt>
<dd><p>Sennrich, R., Haddow, B., &amp; Birch, A. (2016). Neural machine translation of rare words with subword units. <em>Annual Meeting of the Association for Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id296"><span class="brackets">Sha et al., 2023</span></dt>
<dd><p>Sha, Z., He, X., Yu, N., Backes, M., &amp; Zhang, Y. (2023). Can't steal? cont-steal! contrastive stealing attacks against image encoders. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">Shafahi et al., 2018</span></dt>
<dd><p>Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., &amp; Goldstein, T. (2018). Poison frogs! targeted clean-label poisoning attacks on neural networks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id223"><span class="brackets">Shafahi et al., 2019</span></dt>
<dd><p>Shafahi, A., Najibi, M., Ghiasi, M. A., Xu, Z., Dickerson, J., Studer, C., … Goldstein, T. (2019). Adversarial training for free! <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id271"><span class="brackets">Shao et al., 2022</span></dt>
<dd><p>Shao, R., Shi, Z., Yi, J., Chen, P.-Y., &amp; Hsieh, C.-J. (2022). On the adversarial robustness of vision transformers. <em>Transactions on Machine Learning Research</em>.</p>
</dd>
<dt class="label" id="id131"><span class="brackets">Sharif et al., 2016</span></dt>
<dd><p>Sharif, M., Bhagavatula, S., Bauer, L., &amp; Reiter, M. K. (2016). Accessorize to a crime: real and stealthy attacks on state-of-the-art face recognition. <em>ACM SIGSAC Conference on Computer and Communications Security</em>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">Sharma et al., 2018</span></dt>
<dd><p>Sharma, P., Ding, N., Goodman, S., &amp; Soricut, R. (2018). Conceptual captions: a cleaned, hypernymed, image alt-text dataset for automatic image captioning. <em>Annual Meeting of the Association for Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id385"><span class="brackets">Shayegani et al., 2023</span></dt>
<dd><p>Shayegani, E., Dong, Y., &amp; Abu-Ghazaleh, N. (2023). Plug and pray: exploiting off-the-shelf components of multi-modal models. <em>arXiv:2307.14539</em>.</p>
</dd>
<dt class="label" id="id136"><span class="brackets">Shen et al., 2016</span></dt>
<dd><p>Shen, S., Tople, S., &amp; Saxena, P. (2016). Auror: defending against poisoning attacks in collaborative deep learning systems. <em>Conference on Computer Security Applications</em>.</p>
</dd>
<dt class="label" id="id224"><span class="brackets">Shen &amp; Sanghavi, 2019</span></dt>
<dd><p>Shen, Y., &amp; Sanghavi, S. (2019). Learning with bad training data via iterative trimmed loss minimization. <em>International Conference on Machine Learning</em> (pp. 5739–5748).</p>
</dd>
<dt class="label" id="id277"><span class="brackets">Shi et al., 2022</span></dt>
<dd><p>Shi, Y., Han, Y., Tan, Y.-a., &amp; Kuang, X. (2022). Decision-based black-box attack against vision transformers via patch-wise adversarial removal. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id329"><span class="brackets">Shin et al., 2020</span></dt>
<dd><p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., &amp; Singh, S. (2020). Autoprompt: eliciting knowledge from language models with automatically generated prompts. <em>Conference on Empirical Methods in Natural Language Processing</em>.</p>
</dd>
<dt class="label" id="id135"><span class="brackets">Shokri et al., 2017</span></dt>
<dd><p>Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017). Membership inference attacks against machine learning models. <em>IEEE Symposium on Security and Privacy</em>.</p>
</dd>
<dt class="label" id="id225"><span class="brackets">Smith &amp; Topin, 2019</span></dt>
<dd><p>Smith, L. N., &amp; Topin, N. (2019). Super-convergence: very fast training of residual networks using large learning rates. <em>Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</em> (pp. 369–386).</p>
</dd>
<dt class="label" id="id433"><span class="brackets">Smith, 2007</span></dt>
<dd><p>Smith, R. (2007). An overview of the tesseract ocr engine. <em>International Conference on Document Analysis and Recognition</em>.</p>
</dd>
<dt class="label" id="id418"><span class="brackets">Somepalli et al., 2022</span></dt>
<dd><p>Somepalli, G., Singla, V., Goldblum, M., Geiping, J., &amp; Goldstein, T. (2022). Diffusion art or digital forgery? Investigating data replication in diffusion models. <em>arXiv preprint arXiv:2212.03860</em>.</p>
</dd>
<dt class="label" id="id420"><span class="brackets">Somepalli et al., 2023</span></dt>
<dd><p>Somepalli, G., Singla, V., Goldblum, M., Geiping, J., &amp; Goldstein, T. (2023). Understanding data replication in diffusion models. <em>International Conference on Machine Learning WorkShop</em>.</p>
</dd>
<dt class="label" id="id434"><span class="brackets">Song et al., 2020</span></dt>
<dd><p>Song, J., Meng, C., &amp; Ermon, S. (2020). Denoising diffusion implicit models. <em>arXiv preprint arXiv:2010.02502</em>.</p>
</dd>
<dt class="label" id="id226"><span class="brackets">Song et al., 2013</span></dt>
<dd><p>Song, S., Chaudhuri, K., &amp; Sarwate, A. D. (2013). Stochastic gradient descent with differentially private updates. <em>IEEE Global Conference on Signal and Information Processing</em>.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">Sorokin &amp; Forsyth, 2008</span></dt>
<dd><p>Sorokin, A., &amp; Forsyth, D. (2008). Utility data annotation with amazon mechanical turk. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id228"><span class="brackets">Srivastava et al., 2014</span></dt>
<dd><p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. <em>Journal of Machine Learning Research</em>, <em>15</em>(1), 1929–1958.</p>
</dd>
<dt class="label" id="id291"><span class="brackets">Subramanya et al., 2024</span></dt>
<dd><p>Subramanya, A., Koohpayegani, S. A., Saha, A., Tejankar, A., &amp; Pirsiavash, H. (2024). A closer look at robustness of vision transformers to backdoor attacks. <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em> (pp. 3874–3883).</p>
</dd>
<dt class="label" id="id298"><span class="brackets">Subramanya et al., 2022</span></dt>
<dd><p>Subramanya, A., Saha, A., Koohpayegani, S. A., Tejankar, A., &amp; Pirsiavash, H. (2022). Backdoor attacks on vision transformers. <em>arXiv preprint arXiv:2206.08477</em>.</p>
</dd>
<dt class="label" id="id310"><span class="brackets">Sun et al., 2023</span></dt>
<dd><p>Sun, X., Li, X., Meng, Y., Ao, X., Lyu, L., Li, J., &amp; Zhang, T. (2023). Defending against backdoor attacks in natural language generation. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id138"><span class="brackets">Sun et al., 2019</span></dt>
<dd><p>Sun, Z., Kairouz, P., Suresh, A. T., &amp; McMahan, H. B. (2019). <em>Can you really backdoor federated learning?</em></p>
</dd>
<dt class="label" id="id412"><span class="brackets">Sur et al., 2023</span></dt>
<dd><p>Sur, I., Sikka, K., Walmer, M., Koneripalli, K., Roy, A., Lin, X., … Jha, S. (2023). Tijo: trigger inversion with joint optimization for defending multimodal backdoored models. <em>IEEE/CVF International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">Szegedy et al., 2014</span></dt>
<dd><p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp; Fergus, R. (2014). Intriguing properties of neural networks. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id229"><span class="brackets">Szyller et al., 2021</span></dt>
<dd><p>Szyller, S., Atli, B. G., Marchal, S., &amp; Asokan, N. (2021). Dawn: dynamic adversarial watermarking of neural networks. <em>ACM International Conference on Multimedia</em>.</p>
</dd>
<dt class="label" id="id230"><span class="brackets">Tan &amp; Le, 2019</span></dt>
<dd><p>Tan, M., &amp; Le, Q. (2019). Efficientnet: rethinking model scaling for convolutional neural networks. <em>International Conference on Machine Learning</em> (pp. 6105–6114).</p>
</dd>
<dt class="label" id="id108"><span class="brackets">Tang et al., 2020</span></dt>
<dd><p>Tang, R., Du, M., Liu, N., Yang, F., &amp; Hu, X. (2020). An embarrassingly simple approach for trojan attack in deep neural networks. <em>ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (pp. 218–228).</p>
</dd>
<dt class="label" id="id20"><span class="brackets">Taori et al., 2023</span></dt>
<dd><p>Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., … Hashimoto, T. B. (2023). Alpaca: a strong, replicable instruction-following model. <em>Stanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html</em>, <em>3</em>(6), 7.</p>
</dd>
<dt class="label" id="id295"><span class="brackets">Tejankar et al., 2023</span></dt>
<dd><p>Tejankar, A., Sanjabi, M., Wang, Q., Wang, S., Firooz, H., Pirsiavash, H., &amp; Tan, L. (2023). Defending against patch-based backdoor attacks on self-supervised learning. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id299"><span class="brackets">Thies et al., 2016</span></dt>
<dd><p>Thies, J., Zollhöfer, M., Stamminger, M., Theobalt, C., &amp; Nießner, M. (2016). Face2face: real-time face capture and reenactment of rgb videos. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id231"><span class="brackets">Tian et al., 2018</span></dt>
<dd><p>Tian, S., Yang, G., &amp; Cai, Y. (2018). Detecting adversarial examples through image transformation. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">Touvron et al., 2023</span></dt>
<dd><p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., … others. (2023). Llama: open and efficient foundation language models. <em>arXiv preprint arXiv:2302.13971</em>.</p>
</dd>
<dt class="label" id="id233"><span class="brackets">Tramer et al., 2020</span></dt>
<dd><p>Tramer, F., Carlini, N., Brendel, W., &amp; Madry, A. (2020). On adaptive attacks to adversarial example defenses. <em>Advances in Neural Information Processing Systems</em> (pp. 1633–1645).</p>
</dd>
<dt class="label" id="id232"><span class="brackets">Tramer et al., 2018</span></dt>
<dd><p>Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., &amp; McDaniel, P. (2018). Ensemble adversarial training: attacks and defenses. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id114"><span class="brackets">Tramer et al., 2016</span></dt>
<dd><p>Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., &amp; Ristenpart, T. (2016). Stealing machine learning models via prediction $\$APIs$\$. <em>USENIX Security Symposium</em> (pp. 601–618).</p>
</dd>
<dt class="label" id="id234"><span class="brackets">Tran et al., 2018</span></dt>
<dd><p>Tran, B., Li, J., &amp; Madry, A. (2018). Spectral signatures in backdoor attacks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">Tu et al., 2019</span></dt>
<dd><p>Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi, J., … Cheng, S.-M. (2019). Autozoom: autoencoder-based zeroth order optimization method for attacking black-box neural networks. <em>AAAI Conference on Artificial Intelligence</em> (pp. 742–749).</p>
</dd>
<dt class="label" id="id133"><span class="brackets">Turner et al., 2018</span></dt>
<dd><p>Turner, A., Tsipras, D., &amp; Madry, A. (2018). Clean-label backdoor attacks.</p>
</dd>
<dt class="label" id="id235"><span class="brackets">Uchida et al., 2017</span></dt>
<dd><p>Uchida, Y., Nagai, Y., Sakazawa, S., &amp; Satoh, Shin'ichi. (2017). Embedding watermarks into deep neural networks. <em>ACM on International Conference on Multimedia Retrieval</em>.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">Vaswani et al., 2017</span></dt>
<dd><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id115"><span class="brackets">Wang &amp; Gong, 2018</span></dt>
<dd><p>Wang, B., &amp; Gong, N. Z. (2018). Stealing hyperparameters in machine learning. <em>IEEE Symposium on Security and Privacy</em> (pp. 36–52).</p>
</dd>
<dt class="label" id="id242"><span class="brackets">Wang et al., 2019a</span></dt>
<dd><p>Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., &amp; Zhao, B. Y. (2019). Neural cleanse: identifying and mitigating backdoor attacks in neural networks. <em>IEEE Symposium on Security and Privacy</em> (pp. 707–723).</p>
</dd>
<dt class="label" id="id239"><span class="brackets">Wang et al., 2017</span></dt>
<dd><p>Wang, D., Ye, M., &amp; Xu, J. (2017). Differentially private empirical risk minimization revisited: faster and more general. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id113"><span class="brackets">Wang et al., 2020a</span></dt>
<dd><p>Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., Sohn, J.-y., … Papailiopoulos, D. (2020). Attack of the tails: yes, you really can backdoor federated learning. <em>Advances in Neural Information Processing Systems</em> (pp. 16070–16084).</p>
</dd>
<dt class="label" id="id389"><span class="brackets">Wang et al., 2024a</span></dt>
<dd><p>Wang, R., Ma, X., Zhou, H., Ji, C., Ye, G., &amp; Jiang, Y.-G. (2024). White-box multimodal jailbreaks against large vision-language models. <em>arXiv:2405.17894</em>.</p>
</dd>
<dt class="label" id="id244"><span class="brackets">Wang et al., 2022</span></dt>
<dd><p>Wang, S., Nepal, S., Abuadbba, A., Rudolph, C., &amp; Grobler, M. (2022). Adversarial detection by latent style transformations. <em>IEEE Transactions on Information Forensics and Security</em>, <em>17</em>, 1099–1114.</p>
</dd>
<dt class="label" id="id110"><span class="brackets">Wang et al., 2020b</span></dt>
<dd><p>Wang, S., Nepal, S., Rudolph, C., Grobler, M., Chen, S., &amp; Chen, T. (2020). Backdoor attacks against transfer learning with pre-trained deep learning models. <em>IEEE Transactions on Services Computing</em>.</p>
</dd>
<dt class="label" id="id378"><span class="brackets">Wang et al., 2023a</span></dt>
<dd><p>Wang, X., Ji, Z., Ma, P., Li, Z., &amp; Wang, S. (2023). Instructta: instruction-tuned targeted attack for large vision-language models. <em>arXiv:2312.01886</em>.</p>
</dd>
<dt class="label" id="id240"><span class="brackets">Wang et al., 2019b</span></dt>
<dd><p>Wang, Y., Ma, X., Bailey, J., Yi, J., Zhou, B., &amp; Gu, Q. (2019). On the convergence and robustness of adversarial training. <em>International Conference on Machine Learning</em> (pp. 6586–6595).</p>
</dd>
<dt class="label" id="id241"><span class="brackets">Wang et al., 2019c</span></dt>
<dd><p>Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., &amp; Gu, Q. (2019). Improving adversarial robustness requires revisiting misclassified examples. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id245"><span class="brackets">Wang et al., 2023b</span></dt>
<dd><p>Wang, Z., Pang, T., Du, C., Lin, M., Liu, W., &amp; Yan, S. (2023). Better diffusion models further improve adversarial training. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id282"><span class="brackets">Wang et al., 2024b</span></dt>
<dd><p>Wang, Z., Li, X., Zhu, H., &amp; Xie, C. (2024). Revisiting adversarial training at scale. <em>arXiv:2401.04727</em>.</p>
</dd>
<dt class="label" id="id435"><span class="brackets">Wang et al., 2004</span></dt>
<dd><p>Wang, Z., Bovik, A. C., Sheikh, H. R., &amp; Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. <em>IEEE Transactions on Image Processing</em>, <em>13</em>(4), 600–612.</p>
</dd>
<dt class="label" id="id427"><span class="brackets">Webster, 2023</span></dt>
<dd><p>Webster, R. (2023). A reproducible extraction of training images from diffusion models. <em>arXiv preprint arXiv:2305.08694</em>.</p>
</dd>
<dt class="label" id="id344"><span class="brackets">Wei et al., 2021</span></dt>
<dd><p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., … Le, Q. V. (2021). Finetuned language models are zero-shot learners. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id345"><span class="brackets">Wei et al., 2022a</span></dt>
<dd><p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … others. (2022). Chain-of-thought prompting elicits reasoning in large language models. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id406"><span class="brackets">Wei &amp; Zou, 2019</span></dt>
<dd><p>Wei, J., &amp; Zou, K. (2019). Eda: easy data augmentation techniques for boosting performance on text classification tasks. <em>Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</em>.</p>
</dd>
<dt class="label" id="id273"><span class="brackets">Wei et al., 2022b</span></dt>
<dd><p>Wei, Z., Chen, J., Goldblum, M., Wu, Z., Goldstein, T., &amp; Jiang, Y.-G. (2022). Towards transferable adversarial attacks on vision transformers. <em>AAAI Conference on Artificial Intelligence</em> (pp. 2668–2676).</p>
</dd>
<dt class="label" id="id428"><span class="brackets">Wen et al., 2024</span></dt>
<dd><p>Wen, Y., Liu, Y., Chen, C., &amp; Lyu, L. (2024). Detecting, explaining, and mitigating memorization in diffusion models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">Williams &amp; Peng, 1990</span></dt>
<dd><p>Williams, R. J., &amp; Peng, J. (1990). An efficient gradient-based algorithm for on-line training of recurrent network trajectories. <em>Neural Computation</em>, <em>2</em>, 490–501.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">Williams &amp; Zipser, 2013</span></dt>
<dd><p>Williams, R. J., &amp; Zipser, D. (2013). Gradient-based learning algorithms for recurrent networks and their computational complexity. <em>Backpropagation</em> (pp. 433–486). Psychology Press.</p>
</dd>
<dt class="label" id="id246"><span class="brackets">Wong et al., 2020</span></dt>
<dd><p>Wong, E., Rice, L., &amp; Kolter, J. Z. (2020). Fast is better than free: revisiting adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">Wu et al., 2023a</span></dt>
<dd><p>Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., &amp; Duan, N. (2023). Visual chatgpt: talking, drawing and editing with visual foundation models. <em>arXiv preprint arXiv:2303.04671</em>.</p>
</dd>
<dt class="label" id="id248"><span class="brackets">Wu &amp; Wang, 2021</span></dt>
<dd><p>Wu, D., &amp; Wang, Y. (2021). Adversarial neuron pruning purifies backdoored deep models. <em>Advances in Neural Information Processing Systems</em> (pp. 16913–16925).</p>
</dd>
<dt class="label" id="id130"><span class="brackets">Wu et al., 2020a</span></dt>
<dd><p>Wu, D., Wang, Y., Xia, S.-T., Bailey, J., &amp; Ma, X. (2020). Skip connections matter: on the transferability of adversarial examples generated with resnets. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id247"><span class="brackets">Wu et al., 2020b</span></dt>
<dd><p>Wu, D., Xia, S.-T., &amp; Wang, Y. (2020). Adversarial weight perturbation helps robust generalization. <em>Advances in Neural Information Processing Systems</em> (pp. 2958–2969).</p>
</dd>
<dt class="label" id="id297"><span class="brackets">Wu et al., 2023b</span></dt>
<dd><p>Wu, S., Ma, C., Wei, K., Xu, X., Ding, M., Qian, Y., &amp; Xiang, T. (2023). Refine, discriminate and align: stealing encoders via sample-wise prototypes and multi-relational extraction. <em>arXiv preprint arXiv:2312.00855</em>.</p>
</dd>
<dt class="label" id="id323"><span class="brackets">Xi et al., 2024</span></dt>
<dd><p>Xi, Z., Du, T., Li, C., Pang, R., Ji, S., Chen, J., … Wang, T. (2024). Defending pre-trained language models as few-shot learners against backdoor attacks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id311"><span class="brackets">Xiang et al., 2024</span></dt>
<dd><p>Xiang, Z., Jiang, F., Xiong, Z., Ramasubramanian, B., Poovendran, R., &amp; Li, B. (2024). Badchain: backdoor chain-of-thought prompting for large language models. <em>arXiv preprint arXiv:2401.12242</em>.</p>
</dd>
<dt class="label" id="id71"><span class="brackets">Xiao et al., 2018</span></dt>
<dd><p>Xiao, C., Li, B., Zhu, J. Y., He, W., Liu, M., &amp; Song, D. (2018). Generating adversarial examples with adversarial networks. <em>International Joint Conference on Artificial Intelligence</em> (pp. 3905–3911).</p>
</dd>
<dt class="label" id="id112"><span class="brackets">Xie et al., 2019a</span></dt>
<dd><p>Xie, C., Huang, K., Chen, P.-Y., &amp; Li, B. (2019). Dba: distributed backdoor attacks against federated learning. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id251"><span class="brackets">Xie et al., 2020</span></dt>
<dd><p>Xie, C., Tan, M., Gong, B., Yuille, A., &amp; Le, Q. V. (2020). <em>Smooth adversarial training</em>.</p>
</dd>
<dt class="label" id="id249"><span class="brackets">Xie et al., 2018</span></dt>
<dd><p>Xie, C., Wang, J., Zhang, Z., Ren, Z., &amp; Yuille, A. (2018). Mitigating adversarial effects through randomization. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id250"><span class="brackets">Xie et al., 2019b</span></dt>
<dd><p>Xie, C., Wu, Y., Maaten, L. v. d., Yuille, A. L., &amp; He, K. (2019). Feature denoising for improving adversarial robustness. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 501–509).</p>
</dd>
<dt class="label" id="id82"><span class="brackets">Xie et al., 2019c</span></dt>
<dd><p>Xie, C., Zhang, Z., Zhou, Y., Bai, S., Wang, J., Ren, Z., &amp; Yuille, A. L. (2019). Improving transferability of adversarial examples with input diversity. <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2730–2739).</p>
</dd>
<dt class="label" id="id87"><span class="brackets">Xu et al., 2020</span></dt>
<dd><p>Xu, K., Zhang, G., Liu, S., Fan, Q., Sun, M., Chen, H., … Lin, X. (2020). Adversarial t-shirt! evading person detectors in a physical world. <em>European Conference on Computer Vision</em> (pp. 665–681).</p>
</dd>
<dt class="label" id="id252"><span class="brackets">Xu et al., 2018</span></dt>
<dd><p>Xu, W., Evans, D., &amp; Qi, Y. (2018). Feature squeezing: detecting adversarial examples in deep neural networks. <em>Network and Distributed Systems Security Symposium</em>.</p>
</dd>
<dt class="label" id="id279"><span class="brackets">Xu et al., 2023</span></dt>
<dd><p>Xu, X., Zhang, J., &amp; Kankanhalli, M. (2023). Autolora: a parameter-free automated robust fine-tuning framework. <em>arXiv preprint arXiv:2310.01818</em>.</p>
</dd>
<dt class="label" id="id309"><span class="brackets">Yan et al., 2024</span></dt>
<dd><p>Yan, J., Yadav, V., Li, S., Chen, L., Tang, Z., Wang, H., … Jin, H. (2024). Backdooring instruction-tuned large language models with virtual prompt injection. <em>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.</p>
</dd>
<dt class="label" id="id99"><span class="brackets">Yang et al., 2017</span></dt>
<dd><p>Yang, C., Wu, Q., Li, H., &amp; Chen, Y. (2017). <em>Generative poisoning attack method against neural networks</em>.</p>
</dd>
<dt class="label" id="id253"><span class="brackets">Yang et al., 2020</span></dt>
<dd><p>Yang, H., Zhang, J., Dong, H., Inkawhich, N., Gardner, A., Touchet, A., … Li, H. (2020). Dverge: diversifying vulnerabilities for enhanced robust generation of ensembles. <em>Advances in Neural Information Processing Systems</em> (pp. 5505–5515).</p>
</dd>
<dt class="label" id="id42"><span class="brackets">Yang et al., 2019a</span></dt>
<dd><p>Yang, Q., Liu, Y., Chen, T., &amp; Tong, Y. (2019). Federated machine learning: concept and applications. <em>ACM Transactions on Intelligent Systems and Technology</em>, <em>10</em>, 1–19.</p>
</dd>
<dt class="label" id="id405"><span class="brackets">Yang et al., 2023a</span></dt>
<dd><p>Yang, W., Gao, J., &amp; Mirzasoleiman, B. (2023). Better safe than sorry: pre-training clip against targeted data poisoning and backdoor attacks. <em>arXiv preprint arXiv:2310.05862</em>.</p>
</dd>
<dt class="label" id="id404"><span class="brackets">Yang et al., 2023b</span></dt>
<dd><p>Yang, W., Gao, J., &amp; Mirzasoleiman, B. (2023). Robust contrastive language-image pretraining against data poisoning and backdoor attacks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id388"><span class="brackets">Yang et al., 2023c</span></dt>
<dd><p>Yang, Y., Gao, R., Wang, X., Xu, N., &amp; Xu, Q. (2023). Mma-diffusion: multimodal attack on diffusion models. <em>arXiv:2311.17516</em>.</p>
</dd>
<dt class="label" id="id254"><span class="brackets">Yang et al., 2022</span></dt>
<dd><p>Yang, Y., Liu, T. Y., &amp; Mirzasoleiman, B. (2022). Not all poisons are created equal: robust training against data poisoning. <em>International Conference on Machine Learning</em> (pp. 25154–25165).</p>
</dd>
<dt class="label" id="id122"><span class="brackets">Yang et al., 2019b</span></dt>
<dd><p>Yang, Z., Chang, E.-C., &amp; Liang, Z. (2019). Adversarial neural network inversion via auxiliary knowledge alignment. <em>arXiv preprint arXiv:1902.08552</em>.</p>
</dd>
<dt class="label" id="id398"><span class="brackets">Yang et al., 2023d</span></dt>
<dd><p>Yang, Z., He, X., Li, Z., Backes, M., Humbert, M., Berrang, P., &amp; Zhang, Y. (2023). Data poisoning attacks against multimodal encoders. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id109"><span class="brackets">Yao et al., 2019</span></dt>
<dd><p>Yao, Y., Li, H., Zheng, H., &amp; Zhao, B. Y. (2019). Latent backdoor attacks on deep neural networks. <em>ACM SIGSAC Conference on Computer and Communications Security</em> (pp. 2041–2055).</p>
</dd>
<dt class="label" id="id351"><span class="brackets">Ye et al., 2021</span></dt>
<dd><p>Ye, D., Lin, Y., Huang, Y., &amp; Sun, M. (2021). Tr-bert: dynamic token reduction for accelerating bert inference. <em>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.</p>
</dd>
<dt class="label" id="id361"><span class="brackets">Yeom et al., 2018</span></dt>
<dd><p>Yeom, S., Giacomelli, I., Fredrikson, M., &amp; Jha, S. (2018). Privacy risk in machine learning: analyzing the connection to overfitting. <em>IEEE Computer Security Foundations Workshop</em>.</p>
</dd>
<dt class="label" id="id137"><span class="brackets">Yin et al., 2018</span></dt>
<dd><p>Yin, D., Chen, Y., Kannan, R., &amp; Bartlett, P. (2018). Byzantine-robust distributed learning: towards optimal statistical rates. <em>International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id128"><span class="brackets">Yin et al., 2020</span></dt>
<dd><p>Yin, H., Molchanov, P., Alvarez, J. M., Li, Z., Mallya, A., Hoiem, D., … Kautz, J. (2020). Dreaming to distill: data-free knowledge transfer via deepinversion. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id302"><span class="brackets">Yu et al., 2018</span></dt>
<dd><p>Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., &amp; Sang, N. (2018). Bisenet: bilateral segmentation network for real-time semantic segmentation. <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id307"><span class="brackets">Yu et al., 2020</span></dt>
<dd><p>Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., &amp; Finn, C. (2020). Gradient surgery for multi-task learning. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id285"><span class="brackets">Yuan et al., 2023</span></dt>
<dd><p>Yuan, Z., Zhou, P., Zou, K., &amp; Cheng, Y. (2023). You are catching my attention: are vision transformers bad learners under backdoor attacks? <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 24605–24615).</p>
</dd>
<dt class="label" id="id403"><span class="brackets">Zhai et al., 2023</span></dt>
<dd><p>Zhai, S., Dong, Y., Shen, Q., Pu, S., Fang, Y., &amp; Su, H. (2023). Text-to-image diffusion models can be easily backdoored through multimodal data poisoning. <em>ACM International Conference on Multimedia</em>.</p>
</dd>
<dt class="label" id="id258"><span class="brackets">Zhang et al., 2019a</span></dt>
<dd><p>Zhang, D., Zhang, T., Lu, Y., Zhu, Z., &amp; Dong, B. (2019). You only propagate once: accelerating adversarial training via maximal principle. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id257"><span class="brackets">Zhang et al., 2019b</span></dt>
<dd><p>Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., &amp; Jordan, M. (2019). Theoretically principled trade-off between robustness and accuracy. <em>International Conference on Machine Learning</em> (pp. 7472–7482).</p>
</dd>
<dt class="label" id="id366"><span class="brackets">Zhang et al., 2024a</span></dt>
<dd><p>Zhang, J., Wang, Z., Wang, R., Ma, X., &amp; Jiang, Y.-G. (2024). Enja: ensemble jailbreak on large language models. <em>arXiv preprint arXiv:2408.03603</em>.</p>
</dd>
<dt class="label" id="id256"><span class="brackets">Zhang et al., 2018</span></dt>
<dd><p>Zhang, J., Gu, Z., Jang, J., Wu, H., Stoecklin, M. P., Huang, H., &amp; Molloy, I. (2018). Protecting intellectual property of deep neural networks with watermarking. <em>ACM Asia Conference on Computer and Communications Security</em>.</p>
</dd>
<dt class="label" id="id391"><span class="brackets">Zhang et al., 2024b</span></dt>
<dd><p>Zhang, J., Ma, X., Wang, X., Qiu, L., Wang, J., Jiang, Y.-G., &amp; Sang, J. (2024). Adversarial prompt tuning for vision-language models. <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id374"><span class="brackets">Zhang et al., 2022a</span></dt>
<dd><p>Zhang, J., Yi, Q., &amp; Sang, J. (2022). Towards adversarial attack on vision-language pre-training models. <em>ACM International Conference on Multimedia</em> (pp. 5005–5013).</p>
</dd>
<dt class="label" id="id255"><span class="brackets">Zhang et al., 2017</span></dt>
<dd><p>Zhang, J., Zheng, K., Mou, W., &amp; Wang, L. (2017). <em>Efficient private ERM for smooth objectives</em>.</p>
</dd>
<dt class="label" id="id261"><span class="brackets">Zhang et al., 2020a</span></dt>
<dd><p>Zhang, J., Chen, D., Liao, J., Fang, H., Zhang, W., Zhou, W., … Yu, N. (2020). Model watermarking for image processing networks. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id262"><span class="brackets">Zhang et al., 2021</span></dt>
<dd><p>Zhang, J., Chen, D., Liao, J., Zhang, W., Feng, H., Hua, G., &amp; Yu, N. (2021). Deep model intellectual property protection via deep watermarking. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>
</dd>
<dt class="label" id="id259"><span class="brackets">Zhang et al., 2020b</span></dt>
<dd><p>Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., &amp; Kankanhalli, M. (2020). Attacks which do not kill training make adversarial learning stronger. <em>International Conference on Machine Learning</em> (pp. 11278–11287).</p>
</dd>
<dt class="label" id="id260"><span class="brackets">Zhang et al., 2020c</span></dt>
<dd><p>Zhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., &amp; Kankanhalli, M. (2020). Geometry-aware instance-reweighted adversarial training. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id397"><span class="brackets">Zhang et al., 2024c</span></dt>
<dd><p>Zhang, J., Liu, H., Jia, J., &amp; Gong, N. Z. (2024). Data poisoning based backdoor attacks to contrastive learning. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id431"><span class="brackets">Zhang et al., 2024d</span></dt>
<dd><p>Zhang, M., Yu, N., Wen, R., Backes, M., &amp; Zhang, Y. (2024). Generated distributions are all you need for membership inference attacks against generative models. <em>IEEE/CVF Winter Conference on Applications of Computer Vision</em>.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">Zhang et al., 2022b</span></dt>
<dd><p>Zhang, R., Zhang, W., Fang, R., Gao, P., Li, K., Dai, J., … Li, H. (2022). Tip-adapter: training-free adaption of clip for few-shot classification. <em>European Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id348"><span class="brackets">Zhang et al., 2023</span></dt>
<dd><p>Zhang, S., Zhang, M., Pan, X., &amp; Yang, M. (2023). No-skim: towards efficiency robustness evaluation on skimming-based language models. <em>arXiv preprint arXiv:2312.09494</em>.</p>
</dd>
<dt class="label" id="id319"><span class="brackets">Zhang et al., 2020d</span></dt>
<dd><p>Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., &amp; Artzi, Y. (2020). Bertscore: evaluating text generation with bert. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id304"><span class="brackets">Zhao et al., 2021</span></dt>
<dd><p>Zhao, H., Wei, T., Zhou, W., Zhang, W., Chen, D., &amp; Yu, N. (2021). Multi-attentional deepfake detection. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.</p>
</dd>
<dt class="label" id="id377"><span class="brackets">Zhao et al., 2024</span></dt>
<dd><p>Zhao, Y., Pang, T., Du, C., Yang, X., Li, C., Cheung, N.-M. M., &amp; Lin, M. (2024). On evaluating adversarial robustness of large vision-language models. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id286"><span class="brackets">Zheng et al., 2023</span></dt>
<dd><p>Zheng, M., Lou, Q., &amp; Jiang, L. (2023). Trojvit: trojan insertion in vision transformers. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 4025–4034).</p>
</dd>
<dt class="label" id="id331"><span class="brackets">Zhou et al., 2024a</span></dt>
<dd><p>Zhou, A., Li, B., &amp; Wang, H. (2024). Robust prompt optimization for defending language models against jailbreaking attacks. <em>arXiv preprint arXiv:2401.17263</em>.</p>
</dd>
<dt class="label" id="id321"><span class="brackets">Zhou et al., 2024b</span></dt>
<dd><p>Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., … others. (2024). Lima: less is more for alignment. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id376"><span class="brackets">Zhou et al., 2023a</span></dt>
<dd><p>Zhou, Z., Hu, S., Li, M., Zhang, H., Zhang, Y., &amp; Jin, H. (2023). Advclip: downstream-agnostic adversarial examples in multimodal contrastive learning. <em>ACM International Conference on Multimedia</em>.</p>
</dd>
<dt class="label" id="id275"><span class="brackets">Zhou et al., 2023b</span></dt>
<dd><p>Zhou, Z., Hu, S., Zhao, R., Wang, Q., Zhang, L. Y., Hou, J., &amp; Jin, H. (2023). Downstream-agnostic adversarial examples. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 4345–4355).</p>
</dd>
<dt class="label" id="id281"><span class="brackets">Zhu et al., 2020</span></dt>
<dd><p>Zhu, C., Cheng, Y., Gan, Z., Sun, S., Goldstein, T., &amp; Liu, J. (2020). Freelb: enhanced adversarial training for natural language understanding. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">Zhu et al., 2019</span></dt>
<dd><p>Zhu, C., Huang, W. R., Li, H., Taylor, G., Studer, C., &amp; Goldstein, T. (2019). Transferable clean-label poisoning attacks on deep neural nets. <em>International Conference on Machine Learning</em> (pp. 7614–7623).</p>
</dd>
<dt class="label" id="id368"><span class="brackets">Zhu et al., 2023</span></dt>
<dd><p>Zhu, D., Chen, J., Shen, X., Li, X., &amp; Elhoseiny, M. (2023). Minigpt-4: enhancing vision-language understanding with advanced large language models. <em>arXiv preprint arXiv:2304.10592</em>.</p>
</dd>
<dt class="label" id="id266"><span class="brackets">Zhu et al., 2021</span></dt>
<dd><p>Zhu, J., Yao, J., Han, B., Zhang, J., Liu, T., Niu, G., … Yang, H. (2021). Reliable adversarial distillation with unreliable teachers. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id410"><span class="brackets">Zhu et al., 2024</span></dt>
<dd><p>Zhu, L., Ning, R., Li, J., Xin, C., &amp; Wu, H. (2024). Seer: backdoor detection for vision-language models through searching target text and image trigger jointly. <em>AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id386"><span class="brackets">Zhuang et al., 2023</span></dt>
<dd><p>Zhuang, H., Zhang, Y., &amp; Liu, S. (2023). A pilot study of query-free adversarial attack against stable diffusion. <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 2384–2391).</p>
</dd>
<dt class="label" id="id267"><span class="brackets">Zi et al., 2021</span></dt>
<dd><p>Zi, B., Zhao, S., Ma, X., &amp; Jiang, Y.-G. (2021). Revisiting adversarial robustness distillation: robust soft labels make student better. <em>International Conference on Computer Vision</em>.</p>
</dd>
<dt class="label" id="id320"><span class="brackets">Zou et al., 2023</span></dt>
<dd><p>Zou, A., Wang, Z., Kolter, J. Z., &amp; Fredrikson, M. (2023). Universal and transferable adversarial attacks on aligned language models. <em>arXiv preprint arXiv:2307.15043</em>.</p>
</dd>
<dt class="label" id="id59"><span class="brackets">, 2023</span></dt>
<dd><p>张奇、桂韬、黄萱菁. (2023). <em>自然语言处理导论</em>. 上海: 电子工业出版社.</p>
</dd>
</dl>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="chap-8.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>8. 总结与展望</div>
         </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>